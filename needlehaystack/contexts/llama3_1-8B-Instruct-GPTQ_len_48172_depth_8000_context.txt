July 2010What hard liquor, cigarettes, heroin, and crack have in common is
that they're all more concentrated forms of less addictive predecessors.
Most if not all the things we describe as addictive are.  And the
scary thing is, the process that created them is accelerating.We wouldn't want to stop it.  It's the same process that cures
diseases: technological progress.  Technological progress means
making things do more of what we want.  When the thing we want is
something we want to want, we consider technological progress good.
If some new technique makes solar cells x% more efficient, that
seems strictly better.  When progress concentrates something we
don't want to want—when it transforms opium into heroin—it seems
bad.  But it's the same process at work.
[1]No one doubts this process is accelerating, which means increasing
numbers of things we like will be transformed into things we like
too much.
[2]As far as I know there's no word for something we like too much.
The closest is the colloquial sense of "addictive." That usage has
become increasingly common during my lifetime.  And it's clear why:
there are an increasing number of things we need it for.  At the
extreme end of the spectrum are crack and meth.  Food has been
transformed by a combination of factory farming and innovations in
food processing into something with way more immediate bang for the
buck, and you can see the results in any town in America.  Checkers
and solitaire have been replaced by World of Warcraft and FarmVille.
TV has become much more engaging, and even so it can't compete with Facebook.The world is more addictive than it was 40 years ago.   And unless
the forms of technological progress that produced these things are
subject to different laws than technological progress in general,
the world will get more addictive in the next 40 years than it did
in the last 40.The next 40 years will bring us some wonderful things.  I don't
mean to imply they're all to be avoided.  Alcohol is a dangerous
drug, but I'd rather live in a world with wine than one without.
Most people can coexist with alcohol; but you have to be careful.
More things we like will mean more things we have to be careful
about.Most people won't, unfortunately.  Which means that as the world
becomes more addictive, the two senses in which one can live a
normal life will be driven ever further apart.  One sense of "normal"
is statistically normal: what everyone else does.  The other is the
sense we mean when we talk about the normal operating range of a
piece of machinery: what works best.These two senses are already quite far apart.  Already someone
trying to live well would seem eccentrically abstemious in most of
the US.  That phenomenon is only going to become more pronounced.
You can probably take it as a rule of thumb from now on that if
people don't think you're weird, you're living badly.Societies eventually develop antibodies to addictive new things.
I've seen that happen with cigarettes.  When cigarettes first
appeared, they spread the way an infectious disease spreads through
a previously isolated population.  Smoking rapidly became a
(statistically) normal thing.  There were ashtrays everywhere.  We
had ashtrays in our house when I was a kid, even though neither of
my parents smoked.  You had to for guests.As knowledge spread about the dangers of smoking, customs changed.
In the last 20 years, smoking has been transformed from something
that seemed totally normal into a rather seedy habit: from something
movie stars did in publicity shots to something small huddles of
addicts do outside the doors of office buildings.  A lot of the
change was due to legislation, of course, but the legislation
couldn't have happened if customs hadn't already changed.It took a while though—on the order of 100 years.  And unless the
rate at which social antibodies evolve can increase to match the
accelerating rate at which technological progress throws off new
addictions, we'll be increasingly unable to rely on customs to
protect us.
[3]
Unless we want to be canaries in the coal mine
of each new addiction—the people whose sad example becomes a
lesson to future generations—we'll have to figure out for ourselves
what to avoid and how.  It will actually become a reasonable strategy
(or a more reasonable strategy) to suspect 
everything new.In fact, even that won't be enough.  We'll have to worry not just
about new things, but also about existing things becoming more
addictive.  That's what bit me.  I've avoided most addictions, but
the Internet got me because it became addictive while I was using
it.
[4]Most people I know have problems with Internet addiction.  We're
all trying to figure out our own customs for getting free of it.
That's why I don't have an iPhone, for example; the last thing I
want is for the Internet to follow me out into the world.
[5]
My latest trick is taking long hikes.  I used to think running was a
better form of exercise than hiking because it took less time.  Now
the slowness of hiking seems an advantage, because the longer I
spend on the trail, the longer I have to think without interruption.Sounds pretty eccentric, doesn't it?  It always will when you're
trying to solve problems where there are no customs yet to guide
you.  Maybe I can't plead Occam's razor; maybe I'm simply eccentric.
But if I'm right about the acceleration of addictiveness, then this
kind of lonely squirming to avoid it will increasingly be the fate
of anyone who wants to get things done.  We'll increasingly be
defined by what we say no to.
Notes[1]
Could you restrict technological progress to areas where you
wanted it?  Only in a limited way, without becoming a police state.
And even then your restrictions would have undesirable side effects.
"Good" and "bad" technological progress aren't sharply differentiated,
so you'd find you couldn't slow the latter without also slowing the
former.  And in any case, as Prohibition and the "war on drugs"
show, bans often do more harm than good.[2]
Technology has always been accelerating.  By Paleolithic
standards, technology evolved at a blistering pace in the Neolithic
period.[3]
Unless we mass produce social customs.  I suspect the recent
resurgence of evangelical Christianity in the US is partly a reaction
to drugs.  In desperation people reach for the sledgehammer; if
their kids won't listen to them, maybe they'll listen to God.  But
that solution has broader consequences than just getting kids to
say no to drugs.  You end up saying no to 
science as well.
I worry we may be heading for a future in which only a few people
plot their own itinerary through no-land, while everyone else books
a package tour.  Or worse still, has one booked for them by the
government.[4]
People commonly use the word "procrastination" to describe
what they do on the Internet.  It seems to me too mild to describe
what's happening as merely not-doing-work.  We don't call it
procrastination when someone gets drunk instead of working.[5]
Several people have told me they like the iPad because it
lets them bring the Internet into situations where a laptop would
be too conspicuous.  In other words, it's a hip flask.  (This is
true of the iPhone too, of course, but this advantage isn't as
obvious because it reads as a phone, and everyone's used to those.)Thanks to Sam Altman, Patrick Collison, Jessica Livingston, and
Robert Morris for reading drafts of this.October 2015When I talk to a startup that's been operating for more than 8 or
9 months, the first thing I want to know is almost always the same.
Assuming their expenses remain constant and their revenue growth
is what it has been over the last several months, do they make it to
profitability on the money they have left?  Or to put it more
dramatically, by default do they live or die?The startling thing is how often the founders themselves don't know.
Half the founders I talk to don't know whether they're default alive
or default dead.If you're among that number, Trevor Blackwell has made a handy
calculator you can use to find out.The reason I want to know first whether a startup is default alive
or default dead is that the rest of the conversation depends on the
answer.  If the company is default alive, we can talk about ambitious
new things they could do.  If it's default dead, we probably need
to talk about how to save it.  We know the current trajectory ends
badly.  How can they get off that trajectory?Why do so few founders know whether they're default alive or default
dead?  Mainly, I think, because they're not used to asking that.
It's not a question that makes sense to ask early on, any more than
it makes sense to ask a 3 year old how he plans to support
himself.  But as the company grows older, the question switches from
meaningless to critical.  That kind of switch often takes people
by surprise.I propose the following solution: instead of starting to ask too
late whether you're default alive or default dead, start asking too
early.  It's hard to say precisely when the question switches
polarity.  But it's probably not that dangerous to start worrying
too early that you're default dead, whereas it's very dangerous to
start worrying too late.The reason is a phenomenon I wrote about earlier: the
fatal pinch.
The fatal pinch is default dead + slow growth + not enough
time to fix it.  And the way founders end up in it is by not realizing
that's where they're headed.There is another reason founders don't ask themselves whether they're
default alive or default dead: they assume it will be easy to raise
more money.  But that assumption is often false, and worse still, the
more you depend on it, the falser it becomes.Maybe it will help to separate facts from hopes. Instead of thinking
of the future with vague optimism, explicitly separate the components.
Say "We're default dead, but we're counting on investors to save
us." Maybe as you say that, it will set off the same alarms in your
head that it does in mine.  And if you set off the alarms sufficiently
early, you may be able to avoid the fatal pinch.It would be safe to be default dead if you could count on investors
saving you.  As a rule their interest is a function of
growth.  If you have steep revenue growth, say over 5x a year, you
can start to count on investors being interested even if you're not
profitable.
[1]
But investors are so fickle that you can never
do more than start to count on them.  Sometimes something about your
business will spook investors even if your growth is great.  So no
matter how good your growth is, you can never safely treat fundraising
as more than a plan A. You should always have a plan B as well: you
should know (as in write down) precisely what you'll need to do to
survive if you can't raise more money, and precisely when you'll 
have to switch to plan B if plan A isn't working.In any case, growing fast versus operating cheaply is far from the
sharp dichotomy many founders assume it to be.  In practice there
is surprisingly little connection between how much a startup spends
and how fast it grows.  When a startup grows fast, it's usually
because the product hits a nerve, in the sense of hitting some big
need straight on.  When a startup spends a lot, it's usually because
the product is expensive to develop or sell, or simply because
they're wasteful.If you're paying attention, you'll be asking at this point not just
how to avoid the fatal pinch, but how to avoid being default dead.
That one is easy: don't hire too fast.  Hiring too fast is by far
the biggest killer of startups that raise money.
[2]Founders tell themselves they need to hire in order to grow.  But
most err on the side of overestimating this need rather than
underestimating it.  Why?  Partly because there's so much work to
do.  Naive founders think that if they can just hire enough
people, it will all get done.  Partly because successful startups have
lots of employees, so it seems like that's what one does in order
to be successful.  In fact the large staffs of successful startups
are probably more the effect of growth than the cause.  And
partly because when founders have slow growth they don't want to
face what is usually the real reason: the product is not appealing
enough.Plus founders who've just raised money are often encouraged to
overhire by the VCs who funded them.  Kill-or-cure strategies are
optimal for VCs because they're protected by the portfolio effect.
VCs want to blow you up, in one sense of the phrase or the other.
But as a founder your incentives are different.  You want above all
to survive.
[3]Here's a common way startups die.  They make something moderately
appealing and have decent initial growth. They raise their first
round fairly easily, because the founders seem smart and the idea
sounds plausible. But because the product is only moderately
appealing, growth is ok but not great.  The founders convince
themselves that hiring a bunch of people is the way to boost growth.
Their investors agree.  But (because the product is only moderately
appealing) the growth never comes.  Now they're rapidly running out
of runway.  They hope further investment will save them. But because
they have high expenses and slow growth, they're now unappealing
to investors. They're unable to raise more, and the company dies.What the company should have done is address the fundamental problem:
that the product is only moderately appealing.  Hiring people is
rarely the way to fix that.  More often than not it makes it harder.
At this early stage, the product needs to evolve more than to be
"built out," and that's usually easier with fewer people.
[4]Asking whether you're default alive or default dead may save you
from this.  Maybe the alarm bells it sets off will counteract the
forces that push you to overhire.  Instead you'll be compelled to
seek growth in other ways. For example, by doing
things that don't scale, or by redesigning the product in the
way only founders can.
And for many if not most startups, these paths to growth will be
the ones that actually work.Airbnb waited 4 months after raising money at the end of Y Combinator
before they hired their first employee.  In the meantime the founders
were terribly overworked.  But they were overworked evolving Airbnb
into the astonishingly successful organism it is now.Notes[1]
Steep usage growth will also interest investors.  Revenue
will ultimately be a constant multiple of usage, so x% usage growth
predicts x% revenue growth.  But in practice investors discount
merely predicted revenue, so if you're measuring usage you need a
higher growth rate to impress investors.[2]
Startups that don't raise money are saved from hiring too
fast because they can't afford to. But that doesn't mean you should
avoid raising money in order to avoid this problem, any more than
that total abstinence is the only way to avoid becoming an alcoholic.[3]
I would not be surprised if VCs' tendency to push founders
to overhire is not even in their own interest.  They don't know how
many of the companies that get killed by overspending might have
done well if they'd survived.  My guess is a significant number.[4]
After reading a draft, Sam Altman wrote:"I think you should make the hiring point more strongly.  I think
it's roughly correct to say that YC's most successful companies
have never been the fastest to hire, and one of the marks of a great
founder is being able to resist this urge."Paul Buchheit adds:"A related problem that I see a lot is premature scaling—founders
take a small business that isn't really working (bad unit economics,
typically) and then scale it up because they want impressive growth
numbers. This is similar to over-hiring in that it makes the business
much harder to fix once it's big, plus they are bleeding cash really
fast."
Thanks to Sam Altman, Paul Buchheit, Joe Gebbia, Jessica Livingston,
and Geoff Ralston for reading drafts of this.

Want to start a startup?  Get funded by
Y Combinator.




November 2009I don't think Apple realizes how badly the App Store approval process
is broken.  Or rather, I don't think they realize how much it matters
that it's broken.The way Apple runs the App Store has harmed their reputation with
programmers more than anything else they've ever done. 
Their reputation with programmers used to be great.
It used to be the most common complaint you heard
about Apple was that their fans admired them too uncritically.
The App Store has changed that.  Now a lot of programmers
have started to see Apple as evil.How much of the goodwill Apple once had with programmers have they
lost over the App Store?  A third?  Half?  And that's just so far.
The App Store is an ongoing karma leak.* * *How did Apple get into this mess?  Their fundamental problem is
that they don't understand software.They treat iPhone apps the way they treat the music they sell through
iTunes.  Apple is the channel; they own the user; if you want to
reach users, you do it on their terms. The record labels agreed,
reluctantly.  But this model doesn't work for software.  It doesn't
work for an intermediary to own the user.  The software business
learned that in the early 1980s, when companies like VisiCorp showed
that although the words "software" and "publisher" fit together,
the underlying concepts don't.  Software isn't like music or books.
It's too complicated for a third party to act as an intermediary
between developer and user.   And yet that's what Apple is trying
to be with the App Store: a software publisher.  And a particularly
overreaching one at that, with fussy tastes and a rigidly enforced
house style.If software publishing didn't work in 1980, it works even less now
that software development has evolved from a small number of big
releases to a constant stream of small ones.  But Apple doesn't
understand that either.  Their model of product development derives
from hardware.  They work on something till they think it's finished,
then they release it.  You have to do that with hardware, but because
software is so easy to change, its design can benefit from evolution.
The standard way to develop applications now is to launch fast and
iterate.  Which means it's a disaster to have long, random delays
each time you release a new version.Apparently Apple's attitude is that developers should be more careful
when they submit a new version to the App Store.  They would say
that.  But powerful as they are, they're not powerful enough to
turn back the evolution of technology.  Programmers don't use
launch-fast-and-iterate out of laziness.  They use it because it
yields the best results.  By obstructing that process, Apple is
making them do bad work, and programmers hate that as much as Apple
would.How would Apple like it if when they discovered a serious bug in
OS X, instead of releasing a software update immediately, they had
to submit their code to an intermediary who sat on it for a month
and then rejected it because it contained an icon they didn't like?By breaking software development, Apple gets the opposite of what
they intended: the version of an app currently available in the App
Store tends to be an old and buggy one.  One developer told me:

  As a result of their process, the App Store is full of half-baked
  applications. I make a new version almost every day that I release
  to beta users. The version on the App Store feels old and crappy.
  I'm sure that a lot of developers feel this way: One emotion is
  "I'm not really proud about what's in the App Store", and it's
  combined with the emotion "Really, it's Apple's fault."

Another wrote:

  I believe that they think their approval process helps users by
  ensuring quality.  In reality, bugs like ours get through all the
  time and then it can take 4-8 weeks to get that bug fix approved,
  leaving users to think that iPhone apps sometimes just don't work.
  Worse for Apple, these apps work just fine on other platforms
  that have immediate approval processes.

Actually I suppose Apple has a third misconception: that all the
complaints about App Store approvals are not a serious problem.
They must hear developers complaining.  But partners and suppliers
are always complaining.  It would be a bad sign if they weren't;
it would mean you were being too easy on them.  Meanwhile the iPhone
is selling better than ever.  So why do they need to fix anything?They get away with maltreating developers, in the short term, because
they make such great hardware.  I just bought a new 27" iMac a
couple days ago.  It's fabulous.  The screen's too shiny, and the
disk is surprisingly loud, but it's so beautiful that you can't
make yourself care.So I bought it, but I bought it, for the first time, with misgivings.
I felt the way I'd feel buying something made in a country with a
bad human rights record.  That was new.  In the past when I bought
things from Apple it was an unalloyed pleasure.  Oh boy!  They make
such great stuff.  This time it felt like a Faustian bargain.  They
make such great stuff, but they're such assholes.  Do I really want
to support this company?* * *Should Apple care what people like me think?  What difference does
it make if they alienate a small minority of their users?There are a couple reasons they should care.  One is that these
users are the people they want as employees.  If your company seems
evil, the best programmers won't work for you.  That hurt Microsoft
a lot starting in the 90s.  Programmers started to feel sheepish
about working there.  It seemed like selling out.  When people from
Microsoft were talking to other programmers and they mentioned where
they worked, there were a lot of self-deprecating jokes about having
gone over to the dark side.  But the real problem for Microsoft
wasn't the embarrassment of the people they hired.  It was the
people they never got.  And you know who got them?  Google and
Apple.  If Microsoft was the Empire, they were the Rebel Alliance.
And it's largely because they got more of the best people that
Google and Apple are doing so much better than Microsoft today.Why are programmers so fussy about their employers' morals?  Partly
because they can afford to be.  The best programmers can work
wherever they want.  They don't have to work for a company they
have qualms about.But the other reason programmers are fussy, I think, is that evil
begets stupidity.  An organization that wins by exercising power
starts to lose the ability to win by doing better work.  And it's
not fun for a smart person to work in a place where the best ideas
aren't the ones that win.  I think the reason Google embraced "Don't
be evil" so eagerly was not so much to impress the outside world
as to inoculate themselves against arrogance.
[1]That has worked for Google so far.  They've become more
bureaucratic, but otherwise they seem to have held true to their
original principles. With Apple that seems less the case.  When you
look at the famous 
1984 ad 
now, it's easier to imagine Apple as the
dictator on the screen than the woman with the hammer.
[2]
In fact, if you read the dictator's speech it sounds uncannily like a
prophecy of the App Store.

  We have triumphed over the unprincipled dissemination of facts.We have created, for the first time in all history, a garden of
  pure ideology, where each worker may bloom secure from the pests
  of contradictory and confusing truths.

The other reason Apple should care what programmers think of them
is that when you sell a platform, developers make or break you.  If
anyone should know this, Apple should.  VisiCalc made the Apple II.And programmers build applications for the platforms they use.  Most
applications—most startups, probably—grow out of personal projects.
Apple itself did.  Apple made microcomputers because that's what
Steve Wozniak wanted for himself.  He couldn't have afforded a
minicomputer. 
[3]
 Microsoft likewise started out making interpreters
for little microcomputers because
Bill Gates and Paul Allen were interested in using them.  It's a
rare startup that doesn't build something the founders use.The main reason there are so many iPhone apps is that so many programmers
have iPhones.  They may know, because they read it in an article,
that Blackberry has such and such market share.  But in practice
it's as if RIM didn't exist. If they're going to build something,
they want to be able to use it themselves, and that means building
an iPhone app.So programmers continue to develop iPhone apps, even though Apple
continues to maltreat them.  They're like someone stuck in an abusive
relationship.  They're so attracted to the iPhone that they can't
leave.  But they're looking for a way out.  One wrote:

  While I did enjoy developing for the iPhone, the control they
  place on the App Store does not give me the drive to develop
  applications as I would like. In fact I don't intend to make any
  more iPhone applications unless absolutely necessary.
[4]

Can anything break this cycle?  No device I've seen so far could.
Palm and RIM haven't a hope.  The only credible contender is Android.
But Android is an orphan; Google doesn't really care about it, not
the way Apple cares about the iPhone.  Apple cares about the iPhone
the way Google cares about search.* * *Is the future of handheld devices one locked down by Apple?  It's
a worrying prospect.  It would be a bummer to have another grim
monoculture like we had in the 1990s.  In 1995, writing software
for end users was effectively identical with writing Windows
applications.  Our horror at that prospect was the single biggest
thing that drove us to start building web apps.At least we know now what it would take to break Apple's lock.
You'd have to get iPhones out of programmers' hands.  If programmers
used some other device for mobile web access, they'd start to develop
apps for that instead.How could you make a device programmers liked better than the iPhone?
It's unlikely you could make something better designed.  Apple
leaves no room there.  So this alternative device probably couldn't
win on general appeal.  It would have to win by virtue of some
appeal it had to programmers specifically.One way to appeal to programmers is with software.  If you
could think of an application programmers had to have, but that
would be impossible in the circumscribed world of the iPhone, 
you could presumably get them to switch.That would definitely happen if programmers started to use handhelds
as development machines—if handhelds displaced laptops the
way laptops displaced desktops.  You need more control of a development
machine than Apple will let you have over an iPhone.Could anyone make a device that you'd carry around in your pocket
like a phone, and yet would also work as a development machine?
It's hard to imagine what it would look like.  But I've learned
never to say never about technology.  A phone-sized device that
would work as a development machine is no more miraculous by present
standards than the iPhone itself would have seemed by the standards
of 1995.My current development machine is a MacBook Air, which I use with
an external monitor and keyboard in my office, and by itself when
traveling.  If there was a version half the size I'd prefer it.
That still wouldn't be small enough to carry around everywhere like
a phone, but we're within a factor of 4 or so.  Surely that gap is
bridgeable.  In fact, let's make it an
RFS. Wanted: 
Woman with hammer.Notes[1]
When Google adopted "Don't be evil," they were still so small
that no one would have expected them to be, yet.
[2]
The dictator in the 1984 ad isn't Microsoft, incidentally;
it's IBM.  IBM seemed a lot more frightening in those days, but
they were friendlier to developers than Apple is now.[3]
He couldn't even afford a monitor.  That's why the Apple
I used a TV as a monitor.[4]
Several people I talked to mentioned how much they liked the
iPhone SDK.  The problem is not Apple's products but their policies.
Fortunately policies are software; Apple can change them instantly
if they want to.  Handy that, isn't it?Thanks to Sam Altman, Trevor Blackwell, Ross Boucher, 
James Bracy, Gabor Cselle,
Patrick Collison, Jason Freedman, John Gruber, Joe Hewitt, Jessica Livingston,
Robert Morris, Teng Siong Ong, Nikhil Pandit, Savraj Singh, and Jared Tame for reading drafts of this.

Want to start a startup?  Get funded by
Y Combinator.




April 2001, rev. April 2003(This article is derived from a talk given at the 2001 Franz
Developer Symposium.)
In the summer of 1995, my friend Robert Morris and I
started a startup called 
Viaweb.  
Our plan was to write
software that would let end users build online stores.
What was novel about this software, at the time, was
that it ran on our server, using ordinary Web pages
as the interface.A lot of people could have been having this idea at the
same time, of course, but as far as I know, Viaweb was
the first Web-based application.  It seemed such
a novel idea to us that we named the company after it:
Viaweb, because our software worked via the Web,
instead of running on your desktop computer.Another unusual thing about this software was that it
was written primarily in a programming language called
Lisp. It was one of the first big end-user
applications to be written in Lisp, which up till then
had been used mostly in universities and research labs. [1]The Secret WeaponEric Raymond has written an essay called "How to Become a Hacker,"
and in it, among other things, he tells would-be hackers what
languages they should learn.  He suggests starting with Python and
Java, because they are easy to learn.  The serious hacker will also
want to learn C, in order to hack Unix, and Perl for system
administration and cgi scripts.  Finally, the truly serious hacker
should consider learning Lisp:

  Lisp is worth learning for the profound enlightenment experience
  you will have when you finally get it; that experience will make
  you a better programmer for the rest of your days, even if you
  never actually use Lisp itself a lot.

This is the same argument you tend to hear for learning Latin.  It
won't get you a job, except perhaps as a classics professor, but
it will improve your mind, and make you a better writer in languages
you do want to use, like English.But wait a minute.  This metaphor doesn't stretch that far.  The
reason Latin won't get you a job is that no one speaks it.  If you
write in Latin, no one can understand you.  But Lisp is a computer
language, and computers speak whatever language you, the programmer,
tell them to.So if Lisp makes you a better programmer, like he says, why wouldn't
you want to use it? If a painter were offered a brush that would
make him a better painter, it seems to me that he would want to
use it in all his paintings, wouldn't he? I'm not trying to make
fun of Eric Raymond here.  On the whole, his advice is good.  What
he says about Lisp is pretty much the conventional wisdom.  But
there is a contradiction in the conventional wisdom:  Lisp will
make you a better programmer, and yet you won't use it.Why not?  Programming languages are just tools, after all.  If Lisp
really does yield better programs, you should use it.  And if it
doesn't, then who needs it?This is not just a theoretical question.  Software is a very
competitive business, prone to natural monopolies.  A company that
gets software written faster and better will, all other things
being equal, put its competitors out of business.  And when you're
starting a startup, you feel this very keenly.  Startups tend to
be an all or nothing proposition.  You either get rich, or you get
nothing.  In a startup, if you bet on the wrong technology, your
competitors will crush you.Robert and I both knew Lisp well, and we couldn't see any reason
not to trust our instincts and go with Lisp.  We knew that everyone
else was writing their software in C++ or Perl.  But we also knew
that that didn't mean anything.  If you chose technology that way,
you'd be running Windows.  When you choose technology, you have to
ignore what other people are doing, and consider only what will
work the best.This is especially true in a startup.  In a big company, you can
do what all the other big companies are doing.  But a startup can't
do what all the other startups do.  I don't think a lot of people
realize this, even in startups.The average big company grows at about ten percent a year.  So if
you're running a big company and you do everything the way the
average big company does it, you can expect to do as well as the
average big company-- that is, to grow about ten percent a year.The same thing will happen if you're running a startup, of course.
If you do everything the way the average startup does it, you should
expect average performance.  The problem here is, average performance
means that you'll go out of business.  The survival rate for startups
is way less than fifty percent.  So if you're running a startup,
you had better be doing something odd.  If not, you're in trouble.Back in 1995, we knew something that I don't think our competitors
understood, and few understand even now:  when you're writing
software that only has to run on your own servers, you can use
any language you want.  When you're writing desktop software,
there's a strong bias toward writing applications in the same
language as the operating system.  Ten years ago, writing applications
meant writing applications in C.  But with Web-based software,
especially when you have the source code of both the language and
the operating system, you can use whatever language you want.This new freedom is a double-edged sword, however.  Now that you
can use any language, you have to think about which one to use.
Companies that try to pretend nothing has changed risk finding that
their competitors do not.If you can use any language, which do you use?  We chose Lisp.
For one thing, it was obvious that rapid development would be
important in this market.  We were all starting from scratch, so
a company that could get new features done before its competitors
would have a big advantage.  We knew Lisp was a really good language
for writing software quickly, and server-based applications magnify
the effect of rapid development, because you can release software
the minute it's done.If other companies didn't want to use Lisp, so much the better.
It might give us a technological edge, and we needed all the help
we could get.  When we started Viaweb, we had no experience in
business.  We didn't know anything about marketing, or hiring
people, or raising money, or getting customers.  Neither of us had
ever even had what you would call a real job.  The only thing we
were good at was writing software.  We hoped that would save us.
Any advantage we could get in the software department, we would
take.So you could say that using Lisp was an experiment.  Our hypothesis
was that if we wrote our software in Lisp, we'd be able to get
features done faster than our competitors, and also to do things
in our software that they couldn't do.  And because Lisp was so
high-level, we wouldn't need a big development team, so our costs
would be lower.  If this were so, we could offer a better product
for less money, and still make a profit.  We would end up getting
all the users, and our competitors would get none, and eventually
go out of business.  That was what we hoped would happen, anyway.What were the results of this experiment?  Somewhat surprisingly,
it worked.  We eventually had many competitors, on the order of
twenty to thirty of them, but none of their software could compete
with ours.  We had a wysiwyg online store builder that ran on the
server and yet felt like a desktop application.  Our competitors
had cgi scripts.  And we were always far ahead of them in features.
Sometimes, in desperation, competitors would try to introduce
features that we didn't have.  But with Lisp our development cycle
was so fast that we could sometimes duplicate a new feature within
a day or two of a competitor announcing it in a press release.  By
the time journalists covering the press release got round to calling
us, we would have the new feature too.It must have seemed to our competitors that we had some kind of
secret weapon-- that we were decoding their Enigma traffic or
something.  In fact we did have a secret weapon, but it was simpler
than they realized.  No one was leaking news of their features to
us.   We were just able to develop software faster than anyone
thought possible.When I was about nine I happened to get hold of a copy of The Day
of the Jackal, by Frederick Forsyth.  The main character is an
assassin who is hired to kill the president of France.  The assassin
has to get past the police to get up to an apartment that overlooks
the president's route.  He walks right by them, dressed up as an
old man on crutches, and they never suspect him.Our secret weapon was similar.  We wrote our software in a weird
AI language, with a bizarre syntax full of parentheses.  For years
it had annoyed me to hear Lisp described that way.  But now it
worked to our advantage.  In business, there is nothing more valuable
than a technical advantage your competitors don't understand.  In
business, as in war, surprise is worth as much as force.And so, I'm a little embarrassed to say, I never said anything
publicly about Lisp while we were working on Viaweb.  We never
mentioned it to the press, and if you searched for Lisp on our Web
site, all you'd find were the titles of two books in my bio.  This
was no accident.  A startup should give its competitors as little
information as possible.  If they didn't know what language our
software was written in, or didn't care, I wanted to keep it that
way.[2]The people who understood our technology best were the customers.
They didn't care what language Viaweb was written in either, but
they noticed that it worked really well.  It let them build great
looking online stores literally in minutes.  And so, by word of
mouth mostly, we got more and more users.  By the end of 1996 we
had about 70 stores online.  At the end of 1997 we had 500.  Six
months later, when Yahoo bought us, we had 1070 users.  Today, as
Yahoo Store, this software continues to dominate its market.  It's
one of the more profitable pieces of Yahoo, and the stores built
with it are the foundation of Yahoo Shopping.  I left Yahoo in
1999, so I don't know exactly how many users they have now, but
the last I heard there were about 20,000.
The Blub ParadoxWhat's so great about Lisp?  And if Lisp is so great, why doesn't
everyone use it?  These sound like rhetorical questions, but actually
they have straightforward answers.  Lisp is so great not because
of some magic quality visible only to devotees, but because it is
simply the most powerful language available.  And the reason everyone
doesn't use it is that programming languages are not merely
technologies, but habits of mind as well, and nothing changes
slower.  Of course, both these answers need explaining.I'll begin with a shockingly controversial statement:  programming
languages vary in power.Few would dispute, at least, that high level languages are more
powerful than machine language.  Most programmers today would agree
that you do not, ordinarily, want to program in machine language.
Instead, you should program in a high-level language, and have a
compiler translate it into machine language for you.  This idea is
even built into the hardware now: since the 1980s, instruction sets
have been designed for compilers rather than human programmers.Everyone knows it's a mistake to write your whole program by hand
in machine language.  What's less often understood is that there
is a more general principle here: that if you have a choice of
several languages, it is, all other things being equal, a mistake
to program in anything but the most powerful one. [3]There are many exceptions to this rule.  If you're writing a program
that has to work very closely with a program written in a certain
language, it might be a good idea to write the new program in the
same language.  If you're writing a program that only has to do
something very simple, like number crunching or bit manipulation,
you may as well use a less abstract language, especially since it
may be slightly faster.  And if you're writing a short, throwaway
program, you may be better off just using whatever language has
the best library functions for the task.  But in general, for
application software, you want to be using the most powerful
(reasonably efficient) language you can get, and using anything
else is a mistake, of exactly the same kind, though possibly in a
lesser degree, as programming in machine language.You can see that machine language is very low level.  But, at least
as a kind of social convention, high-level languages are often all
treated as equivalent.  They're not.  Technically the term "high-level
language" doesn't mean anything very definite.  There's no dividing
line with machine languages on one side and all the high-level
languages on the other.  Languages fall along a continuum [4] of
abstractness, from the most powerful all the way down to machine
languages, which themselves vary in power.Consider Cobol.  Cobol is a high-level language, in the sense that
it gets compiled into machine language.  Would anyone seriously
argue that Cobol is equivalent in power to, say, Python?  It's
probably closer to machine language than Python.Or how about Perl 4?  Between Perl 4 and Perl 5, lexical closures
got added to the language.  Most Perl hackers would agree that Perl
5 is more powerful than Perl 4.  But once you've admitted that,
you've admitted that one high level language can be more powerful
than another.  And it follows inexorably that, except in special
cases, you ought to use the most powerful you can get.This idea is rarely followed to its conclusion, though.  After a
certain age, programmers rarely switch languages voluntarily.
Whatever language people happen to be used to, they tend to consider
just good enough.Programmers get very attached to their favorite languages, and I
don't want to hurt anyone's feelings, so to explain this point I'm
going to use a hypothetical language called Blub.  Blub falls right
in the middle of the abstractness continuum.  It is not the most
powerful language, but it is more powerful than Cobol or machine
language.And in fact, our hypothetical Blub programmer wouldn't use either
of them.  Of course he wouldn't program in machine language.  That's
what compilers are for.  And as for Cobol, he doesn't know how
anyone can get anything done with it.  It doesn't even have x (Blub
feature of your choice).As long as our hypothetical Blub programmer is looking down the
power continuum, he knows he's looking down.  Languages less powerful
than Blub are obviously less powerful, because they're missing some
feature he's used to.  But when our hypothetical Blub programmer
looks in the other direction, up the power continuum, he doesn't
realize he's looking up.  What he sees are merely weird languages.
He probably considers them about equivalent in power to Blub, but
with all this other hairy stuff thrown in as well.  Blub is good
enough for him, because he thinks in Blub.When we switch to the point of view of a programmer using any of
the languages higher up the power continuum, however, we find that
he in turn looks down upon Blub.  How can you get anything done in
Blub? It doesn't even have y.By induction, the only programmers in a position to see all the
differences in power between the various languages are those who
understand the most powerful one.  (This is probably what Eric
Raymond meant about Lisp making you a better programmer.) You can't
trust the opinions of the others, because of the Blub paradox:
they're satisfied with whatever language they happen to use, because
it dictates the way they think about programs.I know this from my own experience, as a high school kid writing
programs in Basic.  That language didn't even support recursion.
It's hard to imagine writing programs without using recursion, but
I didn't miss it at the time.  I thought in Basic.  And I was a
whiz at it.  Master of all I surveyed.The five languages that Eric Raymond recommends to hackers fall at
various points on the power continuum.  Where they fall relative
to one another is a sensitive topic.  What I will say is that I
think Lisp is at the top.  And to support this claim I'll tell you
about one of the things I find missing when I look at the other
four languages.  How can you get anything done in them, I think,
without macros? [5]Many languages have something called a macro.  But Lisp macros are
unique.  And believe it or not, what they do is related to the
parentheses.  The designers of Lisp didn't put all those parentheses
in the language just to be different.  To the Blub programmer, Lisp
code looks weird.  But those parentheses are there for a reason.
They are the outward evidence of a fundamental difference between
Lisp and other languages.Lisp code is made out of Lisp data objects.  And not in the trivial
sense that the source files contain characters, and strings are
one of the data types supported by the language.  Lisp code, after
it's read by the parser, is made of data structures that you can
traverse.If you understand how compilers work, what's really going on is
not so much that Lisp has a strange syntax as that Lisp has no
syntax.  You write programs in the parse trees that get generated
within the compiler when other languages are parsed.  But these
parse trees are fully accessible to your programs.  You can write
programs that manipulate them.  In Lisp, these programs are called
macros.  They are programs that write programs.Programs that write programs?  When would you ever want to do that?
Not very often, if you think in Cobol.  All the time, if you think
in Lisp.  It would be convenient here if I could give an example
of a powerful macro, and say there! how about that?  But if I did,
it would just look like gibberish to someone who didn't know Lisp;
there isn't room here to explain everything you'd need to know to
understand what it meant.  In 
Ansi Common Lisp I tried to move
things along as fast as I could, and even so I didn't get to macros
until page 160.But I think I can give a kind of argument that might be convincing.
The source code of the Viaweb editor was probably about 20-25%
macros.  Macros are harder to write than ordinary Lisp functions,
and it's considered to be bad style to use them when they're not
necessary.  So every macro in that code is there because it has to
be.  What that means is that at least 20-25% of the code in this
program is doing things that you can't easily do in any other
language.  However skeptical the Blub programmer might be about my
claims for the mysterious powers of Lisp, this ought to make him
curious.  We weren't writing this code for our own amusement.  We
were a tiny startup, programming as hard as we could in order to
put technical barriers between us and our competitors.A suspicious person might begin to wonder if there was some
correlation here.  A big chunk of our code was doing things that
are very hard to do in other languages.  The resulting software
did things our competitors' software couldn't do.  Maybe there was
some kind of connection.  I encourage you to follow that thread.
There may be more to that old man hobbling along on his crutches
than meets the eye.Aikido for StartupsBut I don't expect to convince anyone 
(over 25) 
to go out and learn
Lisp.  The purpose of this article is not to change anyone's mind,
but to reassure people already interested in using Lisp-- people
who know that Lisp is a powerful language, but worry because it
isn't widely used.  In a competitive situation, that's an advantage.
Lisp's power is multiplied by the fact that your competitors don't
get it.If you think of using Lisp in a startup, you shouldn't worry that
it isn't widely understood.  You should hope that it stays that
way. And it's likely to.  It's the nature of programming languages
to make most people satisfied with whatever they currently use.
Computer hardware changes so much faster than personal habits that
programming practice is usually ten to twenty years behind the
processor.  At places like MIT they were writing programs in
high-level languages in the early 1960s, but many companies continued
to write code in machine language well into the 1980s.  I bet a
lot of people continued to write machine language until the processor,
like a bartender eager to close up and go home, finally kicked them
out by switching to a risc instruction set.Ordinarily technology changes fast.  But programming languages are
different: programming languages are not just technology, but what
programmers think in.  They're half technology and half religion.[6]
And so the median language, meaning whatever language the median
programmer uses, moves as slow as an iceberg.  Garbage collection,
introduced by Lisp in about 1960, is now widely considered to be
a good thing.  Runtime typing, ditto, is growing in popularity.
Lexical closures, introduced by Lisp in the early 1970s, are now,
just barely, on the radar screen.  Macros, introduced by Lisp in the
mid 1960s, are still terra incognita.Obviously, the median language has enormous momentum.  I'm not
proposing that you can fight this powerful force.  What I'm proposing
is exactly the opposite: that, like a practitioner of Aikido, you
can use it against your opponents.If you work for a big company, this may not be easy.  You will have
a hard time convincing the pointy-haired boss to let you build
things in Lisp, when he has just read in the paper that some other
language is poised, like Ada was twenty years ago, to take over
the world.  But if you work for a startup that doesn't have
pointy-haired bosses yet, you can, like we did, turn the Blub
paradox to your advantage:  you can use technology that your
competitors, glued immovably to the median language, will never be
able to match.If you ever do find yourself working for a startup, here's a handy
tip for evaluating competitors.  Read their job listings.  Everything
else on their site may be stock photos or the prose equivalent,
but the job listings have to be specific about what they want, or
they'll get the wrong candidates.During the years we worked on Viaweb I read a lot of job descriptions.
A new competitor seemed to emerge out of the woodwork every month
or so.  The first thing I would do, after checking to see if they
had a live online demo, was look at their job listings.  After a
couple years of this I could tell which companies to worry about
and which not to.  The more of an IT flavor the job descriptions
had, the less dangerous the company was.  The safest kind were the
ones that wanted Oracle experience.  You never had to worry about
those.  You were also safe if they said they wanted C++ or Java
developers.  If they wanted Perl or Python programmers, that would
be a bit frightening-- that's starting to sound like a company
where the technical side, at least, is run by real hackers.  If I
had ever seen a job posting looking for Lisp hackers, I would have
been really worried.
Notes[1] Viaweb at first had two parts: the editor, written in Lisp,
which people used to build their sites, and the ordering system,
written in C, which handled orders.  The first version was mostly
Lisp, because the ordering system was small.  Later we added two
more modules, an image generator written in C, and a back-office
manager written mostly in Perl.In January 2003, Yahoo released a new version of the editor 
written in C++ and Perl.  It's hard to say whether the program is no
longer written in Lisp, though, because to translate this program
into C++ they literally had to write a Lisp interpreter: the source
files of all the page-generating templates are still, as far as I
know,  Lisp code.  (See Greenspun's Tenth Rule.)[2] Robert Morris says that I didn't need to be secretive, because
even if our competitors had known we were using Lisp, they wouldn't
have understood why:  "If they were that smart they'd already be
programming in Lisp."[3] All languages are equally powerful in the sense of being Turing
equivalent, but that's not the sense of the word programmers care
about. (No one wants to program a Turing machine.)  The kind of
power programmers care about may not be formally definable, but
one way to explain it would be to say that it refers to features
you could only get in the less powerful language by writing an
interpreter for the more powerful language in it. If language A
has an operator for removing spaces from strings and language B
doesn't, that probably doesn't make A more powerful, because you
can probably write a subroutine to do it in B.  But if A supports,
say, recursion, and B doesn't, that's not likely to be something
you can fix by writing library functions.[4] Note to nerds: or possibly a lattice, narrowing toward the top;
it's not the shape that matters here but the idea that there is at
least a partial order.[5] It is a bit misleading to treat macros as a separate feature.
In practice their usefulness is greatly enhanced by other Lisp
features like lexical closures and rest parameters.[6] As a result, comparisons of programming languages either take
the form of religious wars or undergraduate textbooks so determinedly
neutral that they're really works of anthropology.  People who
value their peace, or want tenure, avoid the topic.  But the question
is only half a religious one; there is something there worth
studying, especially if you want to design new languages.

Want to start a startup?  Get funded by
Y Combinator.




October 2014(This essay is derived from a guest lecture in Sam Altman's startup class at
Stanford.  It's intended for college students, but much of it is
applicable to potential founders at other ages.)One of the advantages of having kids is that when you have to give
advice, you can ask yourself "what would I tell my own kids?"  My
kids are little, but I can imagine what I'd tell them about startups
if they were in college, and that's what I'm going to tell you.Startups are very counterintuitive.  I'm not sure why.  Maybe it's
just because knowledge about them hasn't permeated our culture yet.
But whatever the reason, starting a startup is a task where you
can't always trust your instincts.It's like skiing in that way.  When you first try skiing and you
want to slow down, your instinct is to lean back.  But if you lean
back on skis you fly down the hill out of control.  So part of
learning to ski is learning to suppress that impulse.  Eventually
you get new habits, but at first it takes a conscious effort.  At
first there's a list of things you're trying to remember as you
start down the hill.Startups are as unnatural as skiing, so there's a similar list for
startups. Here I'm going to give you the first part of it — the things
to remember if you want to prepare yourself to start a startup.
CounterintuitiveThe first item on it is the fact I already mentioned: that startups
are so weird that if you trust your instincts, you'll make a lot
of mistakes.  If you know nothing more than this, you may at least
pause before making them.When I was running Y Combinator I used to joke that our function
was to tell founders things they would ignore.  It's really true.
Batch after batch, the YC partners warn founders about mistakes
they're about to make, and the founders ignore them, and then come
back a year later and say "I wish we'd listened."Why do the founders ignore the partners' advice?  Well, that's the
thing about counterintuitive ideas: they contradict your intuitions.
They seem wrong.  So of course your first impulse is to disregard
them.  And in fact my joking description is not merely the curse
of Y Combinator but part of its raison d'etre. If founders' instincts
already gave them the right answers, they wouldn't need us.  You
only need other people to give you advice that surprises you. That's
why there are a lot of ski instructors and not many running
instructors.
[1]You can, however, trust your instincts about people.  And in fact
one of the most common mistakes young founders make is not to
do that enough.  They get involved with people who seem impressive,
but about whom they feel some misgivings personally.  Later when
things blow up they say "I knew there was something off about him,
but I ignored it because he seemed so impressive."If you're thinking about getting involved with someone — as a
cofounder, an employee, an investor, or an acquirer — and you
have misgivings about them, trust your gut.  If someone seems
slippery, or bogus, or a jerk, don't ignore it.This is one case where it pays to be self-indulgent. Work with
people you genuinely like, and you've known long enough to be sure.
ExpertiseThe second counterintuitive point is that it's not that important
to know a lot about startups.  The way to succeed in a startup is
not to be an expert on startups, but to be an expert on your users
and the problem you're solving for them.
Mark Zuckerberg didn't succeed because he was an expert on startups.
He succeeded despite being a complete noob at startups, because he
understood his users really well.If you don't know anything about, say, how to raise an angel round,
don't feel bad on that account.  That sort of thing you can learn
when you need to, and forget after you've done it.In fact, I worry it's not merely unnecessary to learn in great
detail about the mechanics of startups, but possibly somewhat
dangerous.  If I met an undergrad who knew all about convertible
notes and employee agreements and (God forbid) class FF stock, I
wouldn't think "here is someone who is way ahead of their peers."
It would set off alarms.  Because another of the characteristic
mistakes of young founders is to go through the motions of starting
a startup.  They make up some plausible-sounding idea, raise money
at a good valuation, rent a cool office, hire a bunch of people.
From the outside that seems like what startups do.  But the next
step after rent a cool office and hire a bunch of people is: gradually
realize how completely fucked they are, because while imitating all
the outward forms of a startup they have neglected the one thing
that's actually essential: making something people want.
GameWe saw this happen so often that we made up a name for it: playing
house.  Eventually I realized why it was happening.  The reason
young founders go through the motions of starting a startup is
because that's what they've been trained to do for their whole lives
up to that point.  Think about what you have to do to get into
college, for example.  Extracurricular activities, check.  Even in
college classes most of the work is as artificial as running laps.I'm not attacking the educational system for being this way. There
will always be a certain amount of fakeness in the work you do when
you're being taught something, and if you measure their performance
it's inevitable that people will exploit the difference to the point
where much of what you're measuring is artifacts of the fakeness.I confess I did it myself in college. I found that in a lot of
classes there might only be 20 or 30 ideas that were the right shape
to make good exam questions.  The way I studied for exams in these
classes was not (except incidentally) to master the material taught
in the class, but to make a list of potential exam questions and
work out the answers in advance. When I walked into the final, the
main thing I'd be feeling was curiosity about which of my questions
would turn up on the exam.  It was like a game.It's not surprising that after being trained for their whole lives
to play such games, young founders' first impulse on starting a
startup is to try to figure out the tricks for winning at this new
game. Since fundraising appears to be the measure of success for
startups (another classic noob mistake), they always want to know what the
tricks are for convincing investors.  We tell them the best way to
convince investors is to make a startup
that's actually doing well, meaning growing fast, and then simply
tell investors so.  Then they want to know what the tricks are for
growing fast.  And we have to tell them the best way to do that is
simply to make something people want.So many of the conversations YC partners have with young founders
begin with the founder asking "How do we..." and the partner replying
"Just..."Why do the founders always make things so complicated?  The reason,
I realized, is that they're looking for the trick.So this is the third counterintuitive thing to remember about
startups: starting a startup is where gaming the system stops
working.  Gaming the system may continue to work if you go to work
for a big company. Depending on how broken the company is, you can
succeed by sucking up to the right people, giving the impression
of productivity, and so on. 
[2]
But that doesn't work with startups.
There is no boss to trick, only users, and all users care about is
whether your product does what they want. Startups are as impersonal
as physics.  You have to make something people want, and you prosper
only to the extent you do.The dangerous thing is, faking does work to some degree on investors.
If you're super good at sounding like you know what you're talking
about, you can fool investors for at least one and perhaps even two
rounds of funding.  But it's not in your interest to.  The company
is ultimately doomed.  All you're doing is wasting your own time
riding it down.So stop looking for the trick. There are tricks in startups, as
there are in any domain, but they are an order of magnitude less
important than solving the real problem. A founder who knows nothing
about fundraising but has made something users love will have an
easier time raising money than one who knows every trick in the
book but has a flat usage graph. And more importantly, the founder
who has made something users love is the one who will go on to
succeed after raising the money.Though in a sense it's bad news in that you're deprived of one of
your most powerful weapons, I think it's exciting that gaming the
system stops working when you start a startup.  It's exciting that
there even exist parts of the world where you win by doing good
work.  Imagine how depressing the world would be if it were all
like school and big companies, where you either have to spend a lot
of time on bullshit things or lose to people who do.
[3]
I would
have been delighted if I'd realized in college that there were parts
of the real world where gaming the system mattered less than others,
and a few where it hardly mattered at all.  But there are, and this
variation is one of the most important things to consider when
you're thinking about your future.  How do you win in each type of
work, and what would you like to win by doing?
[4]
All-ConsumingThat brings us to our fourth counterintuitive point: startups are
all-consuming.  If you start a startup, it will take over your life
to a degree you cannot imagine.  And if your startup succeeds, it
will take over your life for a long time: for several years at the
very least, maybe for a decade, maybe for the rest of your working
life.  So there is a real opportunity cost here.Larry Page may seem to have an enviable life, but there are aspects
of it that are unenviable.  Basically at 25 he started running as
fast as he could and it must seem to him that he hasn't stopped to
catch his breath since.  Every day new shit happens in the Google
empire that only the CEO can deal with, and he, as CEO, has to deal
with it.  If he goes on vacation for even a week, a whole week's
backlog of shit accumulates.  And he has to bear this uncomplainingly,
partly because as the company's daddy he can never show fear or
weakness, and partly because billionaires get less than zero sympathy
if they talk about having difficult lives.  Which has the strange
side effect that the difficulty of being a successful startup founder
is concealed from almost everyone except those who've done it.Y Combinator has now funded several companies that can be called
big successes, and in every single case the founders say the same
thing.  It never gets any easier.  The nature of the problems change.
You're worrying about construction delays at your London office
instead of the broken air conditioner in your studio apartment.
But the total volume of worry never decreases; if anything it
increases.Starting a successful startup is similar to having kids in that
it's like a button you push that changes your life irrevocably.
And while it's truly wonderful having kids, there are a lot of
things that are easier to do before you have them than after.  Many
of which will make you a better parent when you do have kids. And
since you can delay pushing the button for a while, most people in
rich countries do.Yet when it comes to startups, a lot of people seem to think they're
supposed to start them while they're still in college.  Are you
crazy?  And what are the universities thinking?  They go out of
their way to ensure their students are well supplied with contraceptives,
and yet they're setting up entrepreneurship programs and startup
incubators left and right.To be fair, the universities have their hand forced here.  A lot
of incoming students are interested in startups.  Universities are,
at least de facto, expected to prepare them for their careers.  So
students who want to start startups hope universities can teach
them about startups.  And whether universities can do this or not,
there's some pressure to claim they can, lest they lose applicants
to other universities that do.Can universities teach students about startups?  Yes and no.  They
can teach students about startups, but as I explained before, this
is not what you need to know.  What you need to learn about are the
needs of your own users, and you can't do that until you actually
start the company.
[5]
So starting a startup is intrinsically
something you can only really learn by doing it.  And it's impossible
to do that in college, for the reason I just explained: startups
take over your life.  You can't start a startup for real as a
student, because if you start a startup for real you're not a student
anymore. You may be nominally a student for a bit, but you won't even
be that for long.
[6]Given this dichotomy, which of the two paths should you take?  Be
a real student and not start a startup, or start a real startup and
not be a student?  I can answer that one for you. Do not start a
startup in college.  How to start a startup is just a subset of a
bigger problem you're trying to solve: how to have a good life.
And though starting a startup can be part of a good life for a lot
of ambitious people, age 20 is not the optimal time to do it.
Starting a startup is like a brutally fast depth-first search.  Most
people should still be searching breadth-first at 20.You can do things in your early 20s that you can't do as well before
or after, like plunge deeply into projects on a whim and travel
super cheaply with no sense of a deadline.  For unambitious people,
this sort of thing is the dreaded "failure to launch," but for the
ambitious ones it can be an incomparably valuable sort of exploration.
If you start a startup at 20 and you're sufficiently successful,
you'll never get to do it.
[7]Mark Zuckerberg will never get to bum around a foreign country.  He
can do other things most people can't, like charter jets to fly him
to foreign countries. But success has taken a lot of the serendipity
out of his life. Facebook is running him as much as he's running
Facebook. And while it can be very cool to be in the grip of a
project you consider your life's work, there are advantages to
serendipity too, especially early in life.  Among other things it
gives you more options to choose your life's work from.There's not even a tradeoff here. You're not sacrificing anything
if you forgo starting a startup at 20, because you're more likely
to succeed if you wait.  In the unlikely case that you're 20 and
one of your side projects takes off like Facebook did, you'll face
a choice of running with it or not, and it may be reasonable to run
with it.  But the usual way startups take off is for the founders
to make them take off, and it's gratuitously
stupid to do that at 20.
TryShould you do it at any age?  I realize I've made startups sound
pretty hard.  If I haven't, let me try again: starting a startup
is really hard.  What if it's too hard?  How can you tell if you're
up to this challenge?The answer is the fifth counterintuitive point: you can't tell. Your
life so far may have given you some idea what your prospects might
be if you tried to become a mathematician, or a professional football
player.  But unless you've had a very strange life you haven't done
much that was like being a startup founder.
Starting a startup will change you a lot.  So what you're trying
to estimate is not just what you are, but what you could grow into,
and who can do that?For the past 9 years it was my job to predict whether people would
have what it took to start successful startups.  It was easy to
tell how smart they were, and most people reading this will be over
that threshold.  The hard part was predicting how tough and ambitious they would become.  There
may be no one who has more experience at trying to predict that,
so I can tell you how much an expert can know about it, and the
answer is: not much.  I learned to keep a completely open mind about
which of the startups in each batch would turn out to be the stars.The founders sometimes think they know. Some arrive feeling sure
they will ace Y Combinator just as they've aced every one of the (few,
artificial, easy) tests they've faced in life so far.  Others arrive
wondering how they got in, and hoping YC doesn't discover whatever
mistake caused it to accept them.  But there is little correlation
between founders' initial attitudes and how well their companies
do.I've read that the same is true in the military — that the
swaggering recruits are no more likely to turn out to be really
tough than the quiet ones. And probably for the same reason: that
the tests involved are so different from the ones in their previous
lives.If you're absolutely terrified of starting a startup, you probably
shouldn't do it.  But if you're merely unsure whether you're up to
it, the only way to find out is to try.  Just not now.
IdeasSo if you want to start a startup one day, what should you do in
college?  There are only two things you need initially: an idea and
cofounders.  And the m.o. for getting both is the same.  Which leads
to our sixth and last counterintuitive point: that the way to get
startup ideas is not to try to think of startup ideas.I've written a whole essay on this,
so I won't repeat it all here.  But the short version is that if
you make a conscious effort to think of startup ideas, the ideas
you come up with will not merely be bad, but bad and plausible-sounding,
meaning you'll waste a lot of time on them before realizing they're
bad.The way to come up with good startup ideas is to take a step back.
Instead of making a conscious effort to think of startup ideas,
turn your mind into the type that startup ideas form in without any
conscious effort.  In fact, so unconsciously that you don't even
realize at first that they're startup ideas.This is not only possible, it's how Apple, Yahoo, Google, and
Facebook all got started.  None of these companies were even meant
to be companies at first.  They were all just side projects.  The
best startups almost have to start as side projects, because great
ideas tend to be such outliers that your conscious mind would reject
them as ideas for companies.Ok, so how do you turn your mind into the type that startup ideas
form in unconsciously?  (1) Learn a lot about things that matter,
then (2) work on problems that interest you (3) with people you
like and respect.  The third part, incidentally, is how you get
cofounders at the same time as the idea.The first time I wrote that paragraph, instead of "learn a lot about
things that matter," I wrote "become good at some technology." But
that prescription, though sufficient, is too narrow.  What was
special about Brian Chesky and Joe Gebbia was not that they were
experts in technology.  They were good at design, and perhaps even
more importantly, they were good at organizing groups and making
projects happen.  So you don't have to work on technology per se,
so long as you work on problems demanding enough to stretch you.What kind of problems are those?  That is very hard to answer in
the general case.  History is full of examples of young people who
were working on important problems that no
one else at the time thought were important, and in particular
that their parents didn't think were important.  On the other hand,
history is even fuller of examples of parents who thought their
kids were wasting their time and who were right.  So how do you
know when you're working on real stuff?
[8]I know how I know.  Real problems are interesting, and I am
self-indulgent in the sense that I always want to work on interesting
things, even if no one else cares about them (in fact, especially
if no one else cares about them), and find it very hard to make
myself work on boring things, even if they're supposed to be
important.My life is full of case after case where I worked on something just
because it seemed interesting, and it turned out later to be useful
in some worldly way.  Y
Combinator itself was something I only did because it seemed
interesting. So I seem to have some sort of internal compass that
helps me out.  But I don't know what other people have in their
heads. Maybe if I think more about this I can come up with heuristics
for recognizing genuinely interesting problems, but for the moment
the best I can offer is the hopelessly question-begging advice that
if you have a taste for genuinely interesting problems, indulging
it energetically is the best way to prepare yourself for a startup.
And indeed, probably also the best way to live.
[9]But although I can't explain in the general case what counts as an
interesting problem, I can tell you about a large subset of them.
If you think of technology as something that's spreading like a
sort of fractal stain, every moving point on the edge represents
an interesting problem.  So one guaranteed way to turn your mind
into the type that has good startup ideas is to get yourself to the
leading edge of some technology — to cause yourself, as Paul
Buchheit put it, to "live in the future." When you reach that point,
ideas that will seem to other people uncannily prescient will seem
obvious to you.  You may not realize they're startup ideas, but
you'll know they're something that ought to exist.For example, back at Harvard in the mid 90s a fellow grad student
of my friends Robert and Trevor wrote his own voice over IP software.
He didn't mean it to be a startup, and he never tried to turn it
into one.  He just wanted to talk to his girlfriend in Taiwan without
paying for long distance calls, and since he was an expert on
networks it seemed obvious to him that the way to do it was turn
the sound into packets and ship it over the Internet. He never did
any more with his software than talk to his girlfriend, but this
is exactly the way the best startups get started.So strangely enough the optimal thing to do in college if you want
to be a successful startup founder is not some sort of new, vocational
version of college focused on "entrepreneurship." It's the classic
version of college as education for its own sake. If you want to
start a startup after college, what you should do in college is
learn powerful things.  And if you have genuine intellectual
curiosity, that's what you'll naturally tend to do if you just
follow your own inclinations.
[10]The component of entrepreneurship that really matters is domain
expertise.  The way to become Larry Page was to become an expert
on search. And the way to become an expert on search was to be
driven by genuine curiosity, not some ulterior motive.At its best, starting a startup is merely an ulterior motive for
curiosity.  And you'll do it best if you introduce the ulterior
motive toward the end of the process.So here is the ultimate advice for young would-be startup founders,
boiled down to two words: just learn.
Notes[1]
Some founders listen more than others, and this tends to be a
predictor of success. One of the things I
remember about the Airbnbs during YC is how intently they listened.[2]
In fact, this is one of the reasons startups are possible.  If
big companies weren't plagued by internal inefficiencies, they'd
be proportionately more effective, leaving less room for startups.[3]
In a startup you have to spend a lot of time on schleps, but this sort of work is merely
unglamorous, not bogus.[4]
What should you do if your true calling is gaming the system?
Management consulting.[5]
The company may not be incorporated, but if you start to get
significant numbers of users, you've started it, whether you realize
it yet or not.[6]
It shouldn't be that surprising that colleges can't teach
students how to be good startup founders, because they can't teach
them how to be good employees either.The way universities "teach" students how to be employees is to
hand off the task to companies via internship programs.  But you
couldn't do the equivalent thing for startups, because by definition
if the students did well they would never come back.[7]
Charles Darwin was 22 when he received an invitation to travel
aboard the HMS Beagle as a naturalist.  It was only because he was
otherwise unoccupied, to a degree that alarmed his family, that he
could accept it. And yet if he hadn't we probably would not know
his name.[8]
Parents can sometimes be especially conservative in this
department.  There are some whose definition of important problems
includes only those on the critical path to med school.[9]
I did manage to think of a heuristic for detecting whether you
have a taste for interesting ideas: whether you find known boring
ideas intolerable.  Could you endure studying literary theory, or
working in middle management at a large company?[10]
In fact, if your goal is to start a startup, you can stick
even more closely to the ideal of a liberal education than past
generations have. Back when students focused mainly on getting a
job after college, they thought at least a little about how the
courses they took might look to an employer.  And perhaps even
worse, they might shy away from taking a difficult class lest they
get a low grade, which would harm their all-important GPA.  Good
news: users don't care what your GPA
was.  And I've never heard of investors caring either.  Y Combinator
certainly never asks what classes you took in college or what grades
you got in them.
Thanks to Sam Altman, Paul Buchheit, John Collison, Patrick
Collison, Jessica Livingston, Robert Morris, Geoff Ralston, and
Fred Wilson for reading drafts of this.October 2015This will come as a surprise to a lot of people, but in some cases
it's possible to detect bias in a selection process without knowing
anything about the applicant pool.  Which is exciting because among
other things it means third parties can use this technique to detect
bias whether those doing the selecting want them to or not.You can use this technique whenever (a) you have at least
a random sample of the applicants that were selected, (b) their
subsequent performance is measured, and (c) the groups of
applicants you're comparing have roughly equal distribution of ability.How does it work?  Think about what it means to be biased.  What
it means for a selection process to be biased against applicants
of type x is that it's harder for them to make it through.  Which
means applicants of type x have to be better to get selected than
applicants not of type x.
[1]
Which means applicants of type x
who do make it through the selection process will outperform other
successful applicants.  And if the performance of all the successful
applicants is measured, you'll know if they do.Of course, the test you use to measure performance must be a valid
one.  And in particular it must not be invalidated by the bias you're
trying to measure.
But there are some domains where performance can be measured, and
in those detecting bias is straightforward. Want to know if the
selection process was biased against some type of applicant?  Check
whether they outperform the others.  This is not just a heuristic
for detecting bias.  It's what bias means.For example, many suspect that venture capital firms are biased
against female founders. This would be easy to detect: among their
portfolio companies, do startups with female founders outperform
those without?  A couple months ago, one VC firm (almost certainly
unintentionally) published a study showing bias of this type. First
Round Capital found that among its portfolio companies, startups
with female founders outperformed
those without by 63%. 
[2]The reason I began by saying that this technique would come as a
surprise to many people is that we so rarely see analyses of this
type.  I'm sure it will come as a surprise to First Round that they
performed one. I doubt anyone there realized that by limiting their
sample to their own portfolio, they were producing a study not of
startup trends but of their own biases when selecting companies.I predict we'll see this technique used more in the future.  The
information needed to conduct such studies is increasingly available.
Data about who applies for things is usually closely guarded by the
organizations selecting them, but nowadays data about who gets
selected is often publicly available to anyone who takes the trouble
to aggregate it.
Notes[1]
This technique wouldn't work if the selection process looked
for different things from different types of applicants—for
example, if an employer hired men based on their ability but women
based on their appearance.[2]
As Paul Buchheit points out, First Round excluded their most 
successful investment, Uber, from the study.  And while it 
makes sense to exclude outliers from some types of studies, 
studies of returns from startup investing, which is all about 
hitting outliers, are not one of them.
Thanks to Sam Altman, Jessica Livingston, and Geoff Ralston for reading
drafts of this.

Want to start a startup?  Get funded by
Y Combinator.




March 2008, rev. June 2008Technology tends to separate normal from natural.  Our bodies
weren't designed to eat the foods that people in rich countries eat, or
to get so little exercise.  
There may be a similar problem with the way we work: 
a normal job may be as bad for us intellectually as white flour
or sugar is for us physically.I began to suspect this after spending several years working 
with startup founders.  I've now worked with over 200 of them, and I've
noticed a definite difference between programmers working on their
own startups and those working for large organizations.
I wouldn't say founders seem happier, necessarily;
starting a startup can be very stressful. Maybe the best way to put
it is to say that they're happier in the sense that your body is
happier during a long run than sitting on a sofa eating
doughnuts.Though they're statistically abnormal, startup founders seem to be
working in a way that's more natural for humans.I was in Africa last year and saw a lot of animals in the wild that
I'd only seen in zoos before. It was remarkable how different they
seemed. Particularly lions. Lions in the wild seem about ten times
more alive. They're like different animals. I suspect that working
for oneself feels better to humans in much the same way that living
in the wild must feel better to a wide-ranging predator like a lion.
Life in a zoo is easier, but it isn't the life they were designed
for.
TreesWhat's so unnatural about working for a big company?  The root of
the problem is that humans weren't meant to work in such large
groups.Another thing you notice when you see animals in the wild is that
each species thrives in groups of a certain size.  A herd of impalas
might have 100 adults; baboons maybe 20; lions rarely 10.  Humans
also seem designed to work in groups, and what I've read about
hunter-gatherers accords with research on organizations and my own
experience to suggest roughly what the ideal size is: groups of 8
work well; by 20 they're getting hard to manage; and a group of 50
is really unwieldy.
[1]
Whatever the upper limit is, we are clearly not meant to work in
groups of several hundred.  And yet—for reasons having more
to do with technology than human nature—a great many people
work for companies with hundreds or thousands of employees.Companies know groups that large wouldn't work, so they divide
themselves into units small enough to work together.  But to
coordinate these they have to introduce something new: bosses.These smaller groups are always arranged in a tree structure.  Your
boss is the point where your group attaches to the tree.  But when
you use this trick for dividing a large group into smaller ones,
something strange happens that I've never heard anyone mention
explicitly.  In the group one level up from yours, your boss
represents your entire group.  A group of 10 managers is not merely
a group of 10 people working together in the usual way.  It's really
a group of groups.  Which means for a group of 10 managers to work
together as if they were simply a group of 10 individuals, the group
working for each manager would have to work as if they were a single
person—the workers and manager would each share only one
person's worth of freedom between them.In practice a group of people are never able to act as if they were
one person.  But in a large organization divided into groups in
this way, the pressure is always in that direction.  Each group
tries its best to work as if it were the small group of individuals
that humans were designed to work in.  That was the point of creating
it.  And when you propagate that constraint, the result is that
each person gets freedom of action in inverse proportion to the
size of the entire tree.
[2]Anyone who's worked for a large organization has felt this.  You
can feel the difference between working for a company with 100
employees and one with 10,000, even if your group has only 10 people.
Corn SyrupA group of 10 people within a large organization is a kind of fake
tribe.  The number of people you interact with is about right.  But
something is missing: individual initiative.  Tribes of hunter-gatherers
have much more freedom.  The leaders have a little more power than other
members of the tribe, but they don't generally tell them what to
do and when the way a boss can.It's not your boss's fault.  The real problem is that in the group
above you in the hierarchy, your entire group is one virtual person.
Your boss is just the way that constraint is imparted to you.So working in a group of 10 people within a large organization feels
both right and wrong at the same time.   On the surface it feels
like the kind of group you're meant to work in, but something major
is missing.  A job at a big company is like high fructose corn
syrup: it has some of the qualities of things you're meant to like,
but is disastrously lacking in others.Indeed, food is an excellent metaphor to explain what's wrong with
the usual sort of job.For example, working for a big company is the default thing to do,
at least for programmers.  How bad could it be?  Well, food shows
that pretty clearly.  If you were dropped at a random point in
America today, nearly all the food around you would be bad for you.
Humans were not designed to eat white flour, refined sugar, high
fructose corn syrup, and hydrogenated vegetable oil.  And yet if
you analyzed the contents of the average grocery store you'd probably
find these four ingredients accounted for most of the calories.
"Normal" food is terribly bad for you.  The only people who eat
what humans were actually designed to eat are a few Birkenstock-wearing
weirdos in Berkeley.If "normal" food is so bad for us, why is it so common?  There are
two main reasons. One is that it has more immediate appeal.  You
may feel lousy an hour after eating that pizza, but eating the first
couple bites feels great.  The other is economies of scale.
Producing junk food scales; producing fresh vegetables doesn't.
Which means (a) junk food can be very cheap, and (b) it's worth
spending a lot to market it.If people have to choose between something that's cheap, heavily
marketed, and appealing in the short term, and something that's
expensive, obscure, and appealing in the long term, which do you
think most will choose?It's the same with work.  The average MIT graduate wants to work
at Google or Microsoft, because it's a recognized brand, it's safe,
and they'll get paid a good salary right away.  It's the job
equivalent of the pizza they had for lunch.  The drawbacks will
only become apparent later, and then only in a vague sense of
malaise.And founders and early employees of startups, meanwhile, are like
the Birkenstock-wearing weirdos of Berkeley:  though a tiny minority
of the population, they're the ones living as humans are meant to.
In an artificial world, only extremists live naturally.
ProgrammersThe restrictiveness of big company jobs is particularly hard on
programmers, because the essence of programming is to build new
things.  Sales people make much the same pitches every day; support
people answer much the same questions; but once you've written a
piece of code you don't need to write it again.  So a programmer
working as programmers are meant to is always making new things.
And when you're part of an organization whose structure gives each
person freedom in inverse proportion to the size of the tree, you're
going to face resistance when you do something new.This seems an inevitable consequence of bigness.  It's true even
in the smartest companies.  I was talking recently to a founder who
considered starting a startup right out of college, but went to
work for Google instead because he thought he'd learn more there.
He didn't learn as much as he expected.  Programmers learn by doing,
and most of the things he wanted to do, he couldn't—sometimes
because the company wouldn't let him, but often because the company's
code wouldn't let him.  Between the drag of legacy code, the overhead
of doing development in such a large organization, and the restrictions
imposed by interfaces owned by other groups, he could only try a
fraction of the things he would have liked to.  He said he has
learned much more in his own startup, despite the fact that he has
to do all the company's errands as well as programming, because at
least when he's programming he can do whatever he wants.An obstacle downstream propagates upstream.  If you're not allowed
to implement new ideas, you stop having them.  And vice versa: when
you can do whatever you want, you have more ideas about what to do.
So working for yourself makes your brain more powerful in the same
way a low-restriction exhaust system makes an engine more powerful.Working for yourself doesn't have to mean starting a startup, of
course.  But a programmer deciding between a regular job at a big
company and their own startup is probably going to learn more doing
the startup.You can adjust the amount of freedom you get by scaling the size
of company you work for.  If you start the company, you'll have the
most freedom.  If you become one of the first 10 employees you'll
have almost as much freedom as the founders.  Even a company with
100 people will feel different from one with 1000.Working for a small company doesn't ensure freedom.  The tree
structure of large organizations sets an upper bound on freedom,
not a lower bound.  The head of a small company may still choose
to be a tyrant.  The point is that a large organization is compelled
by its structure to be one.
ConsequencesThat has real consequences for both organizations and individuals.
One is that companies will inevitably slow down as they grow larger,
no matter how hard they try to keep their startup mojo.  It's a
consequence of the tree structure that every large organization is
forced to adopt.Or rather, a large organization could only avoid slowing down if
they avoided tree structure.  And since human nature limits the
size of group that can work together, the only way I can imagine
for larger groups to avoid tree structure would be to have no
structure: to have each group actually be independent, and to work
together the way components of a market economy do.That might be worth exploring.  I suspect there are already some
highly partitionable businesses that lean this way.  But I don't
know any technology companies that have done it.There is one thing companies can do short of structuring themselves
as sponges:  they can stay small.  If I'm right, then it really
pays to keep a company as small as it can be at every stage.
Particularly a technology company.  Which means it's doubly important
to hire the best people.  Mediocre hires hurt you twice: they get
less done, but they also make you big, because you need more of
them to solve a given problem.For individuals the upshot is the same: aim small.  It will always
suck to work for large organizations, and the larger the organization,
the more it will suck.In an essay I wrote a couple years ago 
I advised graduating seniors
to work for a couple years for another company before starting their
own.  I'd modify that now.  Work for another company if you want
to, but only for a small one, and if you want to start your own
startup, go ahead.The reason I suggested college graduates not start startups immediately
was that I felt most would fail.  And they will.  But ambitious
programmers are better off doing their own thing and failing than
going to work at a big company.  Certainly they'll learn more.  They
might even be better off financially.  A lot of people in their
early twenties get into debt, because their expenses grow even
faster than the salary that seemed so high when they left school.
At least if you start a startup and fail your net worth will be
zero rather than negative.  
[3]We've now funded so many different types of founders that we have
enough data to see patterns, and there seems to be no benefit from
working for a big company.  The people who've worked for a few years
do seem better than the ones straight out of college, but only
because they're that much older.The people who come to us from big companies often seem kind of
conservative.  It's hard to say how much is because big companies
made them that way, and how much is the natural conservatism that
made them work for the big companies in the first place.  But
certainly a large part of it is learned.  I know because I've seen
it burn off.Having seen that happen so many times is one of the things that
convinces me that working for oneself, or at least for a small
group, is the natural way for programmers to live.  Founders arriving
at Y Combinator often have the downtrodden air of refugees.  Three
months later they're transformed: they have so much more 
confidence
that they seem as if they've grown several inches taller. 
[4]
Strange as this sounds, they seem both more worried and happier at the same
time.  Which is exactly how I'd describe the way lions seem in the
wild.Watching employees get transformed into founders makes it clear
that the difference between the two is due mostly to environment—and
in particular that the environment in big companies is toxic to
programmers.   In the first couple weeks of working on their own
startup they seem to come to life, because finally they're working
the way people are meant to.Notes[1]
When I talk about humans being meant or designed to live a
certain way, I mean by evolution.[2]
It's not only the leaves who suffer.  The constraint propagates
up as well as down.  So managers are constrained too; instead of
just doing things, they have to act through subordinates.[3]
Do not finance your startup with credit cards.  Financing a
startup with debt is usually a stupid move, and credit card debt
stupidest of all.  Credit card debt is a bad idea, period.  It is
a trap set by evil companies for the desperate and the foolish.[4]
The founders we fund used to be younger (initially we encouraged
undergrads to apply), and the first couple times I saw this I used
to wonder if they were actually getting physically taller.Thanks to Trevor Blackwell, Ross Boucher, Aaron Iba, Abby
Kirigin, Ivan Kirigin, Jessica Livingston, and Robert Morris for
reading drafts of this.July 2006
When I was in high school I spent a lot of time imitating bad
writers.  What we studied in English classes was mostly fiction,
so I assumed that was the highest form of writing.  Mistake number
one.  The stories that seemed to be most admired were ones in which
people suffered in complicated ways.  Anything funny or
gripping was ipso facto suspect, unless it was old enough to be hard to
understand, like Shakespeare or Chaucer.  Mistake number two.  The
ideal medium seemed the short story, which I've since learned had
quite a brief life, roughly coincident with the peak of magazine
publishing.  But since their size made them perfect for use in
high school classes, we read a lot of them, which gave us the
impression the short story was flourishing.  Mistake number three.
And because they were so short, nothing really had to happen; you
could just show a randomly truncated slice of life, and that was
considered advanced.  Mistake number four.  The result was that I
wrote a lot of stories in which nothing happened except that someone
was unhappy in a way that seemed deep.For most of college I was a philosophy major.  I was very impressed
by the papers published in philosophy journals.  They were so
beautifully typeset, and their tone was just captivating—alternately
casual and buffer-overflowingly technical.  A fellow would be walking
along a street and suddenly modality qua modality would spring upon
him.  I didn't ever quite understand these papers, but I figured
I'd get around to that later, when I had time to reread them more
closely.  In the meantime I tried my best to imitate them.  This
was, I can now see, a doomed undertaking, because they weren't
really saying anything.  No philosopher ever refuted another, for
example, because no one said anything definite enough to refute.
Needless to say, my imitations didn't say anything either.In grad school I was still wasting time imitating the wrong things.
There was then a fashionable type of program called an expert system,
at the core of which was something called an inference engine.  I
looked at what these things did and thought "I could write that in
a thousand lines of code."  And yet eminent professors were writing
books about them, and startups were selling them for a year's salary
a copy.  What an opportunity, I thought; these impressive things
seem easy to me; I must be pretty sharp.  Wrong.  It was simply a
fad.  The books the professors wrote about expert systems are now
ignored.  They were not even on a path to anything interesting.
And the customers paying so much for them were largely the same
government agencies that paid thousands for screwdrivers and toilet
seats.How do you avoid copying the wrong things?  Copy only what you
genuinely like.  That would have saved me in all three cases.  I
didn't enjoy the short stories we had to read in English classes;
I didn't learn anything from philosophy papers; I didn't use expert
systems myself.  I believed these things were good because they
were admired.It can be hard to separate the things you like from the things
you're impressed with.  One trick is to ignore presentation.  Whenever
I see a painting impressively hung in a museum, I ask myself: how
much would I pay for this if I found it at a garage sale, dirty and
frameless, and with no idea who painted it?  If you walk around a
museum trying this experiment, you'll find you get some truly
startling results.  Don't ignore this data point just because it's
an outlier.Another way to figure out what you like is to look at what you enjoy
as guilty pleasures.  Many things people like, especially if they're
young and ambitious, they like largely for the feeling of virtue
in liking them.  99% of people reading Ulysses are thinking
"I'm reading Ulysses" as they do it. A guilty pleasure is
at least a pure one.  What do you read when you don't feel up to being
virtuous?  What kind of book do you read and feel sad that there's
only half of it left, instead of being impressed that you're half
way through?  That's what you really like.Even when you find genuinely good things to copy, there's another
pitfall to be avoided.  Be careful to copy what makes them good,
rather than their flaws.  It's easy to be drawn into imitating
flaws, because they're easier to see, and of course easier to copy
too.  For example, most painters in the eighteenth and nineteenth
centuries used brownish colors.  They were imitating the great
painters of the Renaissance, whose paintings by that time were brown
with dirt.  Those paintings have since been cleaned, revealing
brilliant colors; their imitators are of course still brown.It was painting, incidentally, that cured me of copying the wrong
things.  Halfway through grad school I decided I wanted to try being
a painter, and the art world was so manifestly corrupt that it
snapped the leash of credulity.  These people made philosophy
professors seem as scrupulous as mathematicians.  It was so clearly
a choice of doing good work xor being an insider that I was forced
to see the distinction.  It's there to some degree in almost every
field, but I had till then managed to avoid facing it.That was one of the most valuable things I learned from painting:
you have to figure out for yourself what's 
good.  You can't trust
authorities. They'll lie to you on this one.

Comment on this essay.January 2015Corporate Development, aka corp dev, is the group within companies
that buys other companies. If you're talking to someone from corp
dev, that's why, whether you realize it yet or not.It's usually a mistake to talk to corp dev unless (a) you want to
sell your company right now and (b) you're sufficiently likely to
get an offer at an acceptable price.  In practice that means startups
should only talk to corp dev when they're either doing really well
or really badly.  If you're doing really badly, meaning the company
is about to die, you may as well talk to them, because you have
nothing to lose. And if you're doing really well, you can safely
talk to them, because you both know the price will have to be high,
and if they show the slightest sign of wasting your time, you'll
be confident enough to tell them to get lost.The danger is to companies in the middle.  Particularly to young
companies that are growing fast, but haven't been doing it for long
enough to have grown big yet.  It's usually a mistake for a promising
company less than a year old even to talk to corp dev.But it's a mistake founders constantly make.  When someone from
corp dev wants to meet, the founders tell themselves they should
at least find out what they want.  Besides, they don't want to
offend Big Company by refusing to meet.Well, I'll tell you what they want.  They want to talk about buying
you.  That's what the title "corp dev" means.   So before agreeing
to meet with someone from corp dev, ask yourselves, "Do we want to
sell the company right now?"  And if the answer is no, tell them
"Sorry, but we're focusing on growing the company."  They won't be
offended.  And certainly the founders of Big Company won't be
offended. If anything they'll think more highly of you.  You'll
remind them of themselves.  They didn't sell either; that's why
they're in a position now to buy other companies.
[1]Most founders who get contacted by corp dev already know what it
means.  And yet even when they know what corp dev does and know
they don't want to sell, they take the meeting.  Why do they do it?
The same mix of denial and wishful thinking that underlies most
mistakes founders make. It's flattering to talk to someone who wants
to buy you.  And who knows, maybe their offer will be surprisingly
high.  You should at least see what it is, right?No.  If they were going to send you an offer immediately by email,
sure, you might as well open it.  But that is not how conversations
with corp dev work.  If you get an offer at all, it will be at the
end of a long and unbelievably distracting process.  And if the
offer is surprising, it will be surprisingly low.Distractions are the thing you can least afford in a startup.  And
conversations with corp dev are the worst sort of distraction,
because as well as consuming your attention they undermine your
morale.  One of the tricks to surviving a grueling process is not
to stop and think how tired you are.  Instead you get into a sort
of flow. 
[2]
Imagine what it would do to you if at mile 20 of a
marathon, someone ran up beside you and said "You must feel really
tired.  Would you like to stop and take a rest?"  Conversations
with corp dev are like that but worse, because the suggestion of
stopping gets combined in your mind with the imaginary high price
you think they'll offer.And then you're really in trouble.  If they can, corp dev people
like to turn the tables on you. They like to get you to the point
where you're trying to convince them to buy instead of them trying
to convince you to sell.  And surprisingly often they succeed.This is a very slippery slope, greased with some of the most powerful
forces that can work on founders' minds, and attended by an experienced
professional whose full time job is to push you down it.Their tactics in pushing you down that slope are usually fairly
brutal. Corp dev people's whole job is to buy companies, and they
don't even get to choose which.  The only way their performance is
measured is by how cheaply they can buy you, and the more ambitious
ones will stop at nothing to achieve that. For example, they'll
almost always start with a lowball offer, just to see if you'll
take it. Even if you don't, a low initial offer will demoralize you
and make you easier to manipulate.And that is the most innocent of their tactics. Just wait till
you've agreed on a price and think you have a done deal, and then
they come back and say their boss has vetoed the deal and won't do
it for more than half the agreed upon price. Happens all the time.
If you think investors can behave badly, it's nothing compared to
what corp dev people can do.  Even corp dev people at companies
that are otherwise benevolent.I remember once complaining to a
friend at Google about some nasty trick their corp dev people had
pulled on a YC startup."What happened to Don't be Evil?" I asked."I don't think corp dev got the memo," he replied.The tactics you encounter in M&A conversations can be like nothing
you've experienced in the otherwise comparatively 
upstanding world
of Silicon Valley.  It's as if a chunk of genetic material from the
old-fashioned robber baron business world got incorporated into the
startup world.
[3]The simplest way to protect yourself is to use the trick that John
D. Rockefeller, whose grandfather was an alcoholic, used to protect
himself from becoming one.  He once told a Sunday school class

  Boys, do you know why I never became a drunkard?  Because I never
  took the first drink.

Do you want to sell your company right now?  Not eventually, right
now.  If not, just don't take the first meeting.  They won't be
offended.  And you in turn will be guaranteed to be spared one of
the worst experiences that can happen to a startup.If you do want to sell, there's another set of 
techniques
 for doing
that.  But the biggest mistake founders make in dealing with corp
dev is not doing a bad job of talking to them when they're ready
to, but talking to them before they are.  So if you remember only
the title of this essay, you already know most of what you need to
know about M&A in the first year.Notes[1]
I'm not saying you should never sell.  I'm saying you should
be clear in your own mind about whether you want to sell or not,
and not be led by manipulation or wishful thinking into trying to
sell earlier than you otherwise would have.[2]
In a startup, as in most competitive sports, the task at hand
almost does this for you; you're too busy to feel tired.  But when
you lose that protection, e.g. at the final whistle, the fatigue
hits you like a wave.  To talk to corp dev is to let yourself feel
it mid-game.[3]
To be fair, the apparent misdeeds of corp dev people are magnified
by the fact that they function as the face of a large organization
that often doesn't know its own mind.  Acquirers can be surprisingly
indecisive about acquisitions, and their flakiness is indistinguishable
from dishonesty by the time it filters down to you.Thanks to Marc Andreessen, Jessica Livingston, Geoff
Ralston, and Qasar Younis for reading drafts of this.January 2003(This article is derived from a keynote talk at the fall 2002 meeting
of NEPLS.)Visitors to this country are often surprised to find that
Americans like to begin a conversation by asking "what do you do?"
I've never liked this question.  I've rarely had a
neat answer to it.  But I think I have finally solved the problem.
Now, when someone asks me what I do, I look them straight
in the eye and say "I'm designing a 
new dialect of Lisp."   
I recommend this answer to anyone who doesn't like being asked what
they do.  The conversation will turn immediately to other topics.I don't consider myself to be doing research on programming languages.
I'm just designing one, in the same way that someone might design
a building or a chair or a new typeface.
I'm not trying to discover anything new.  I just want
to make a language that will be good to program in.  In some ways,
this assumption makes life a lot easier.The difference between design and research seems to be a question
of new versus good.  Design doesn't have to be new, but it has to  
be good.  Research doesn't have to be good, but it has to be new.
I think these two paths converge at the top: the best design
surpasses its predecessors by using new ideas, and the best research
solves problems that are not only new, but actually worth solving.
So ultimately we're aiming for the same destination, just approaching
it from different directions.What I'm going to talk about today is what your target looks like
from the back.  What do you do differently when you treat
programming languages as a design problem instead of a research topic?The biggest difference is that you focus more on the user.
Design begins by asking, who is this
for and what do they need from it?  A good architect,
for example, does not begin by creating a design that he then
imposes on the users, but by studying the intended users and figuring
out what they need.Notice I said "what they need," not "what they want."  I don't mean
to give the impression that working as a designer means working as 
a sort of short-order cook, making whatever the client tells you
to.  This varies from field to field in the arts, but
I don't think there is any field in which the best work is done by
the people who just make exactly what the customers tell them to.The customer is always right in
the sense that the measure of good design is how well it works
for the user.  If you make a novel that bores everyone, or a chair
that's horribly uncomfortable to sit in, then you've done a bad
job, period.  It's no defense to say that the novel or the chair  
is designed according to the most advanced theoretical principles.And yet, making what works for the user doesn't mean simply making
what the user tells you to.  Users don't know what all the choices
are, and are often mistaken about what they really want.The answer to the paradox, I think, is that you have to design
for the user, but you have to design what the user needs, not simply  
what he says he wants.
It's much like being a doctor.  You can't just treat a patient's
symptoms.  When a patient tells you his symptoms, you have to figure
out what's actually wrong with him, and treat that.This focus on the user is a kind of axiom from which most of the
practice of good design can be derived, and around which most design
issues center.If good design must do what the user needs, who is the user?  When
I say that design must be for users, I don't mean to imply that good 
design aims at some kind of  
lowest common denominator.  You can pick any group of users you
want.  If you're designing a tool, for example, you can design it
for anyone from beginners to experts, and what's good design
for one group might be bad for another.  The point
is, you have to pick some group of users.  I don't think you can
even talk about good or bad design except with
reference to some intended user.You're most likely to get good design if the intended users include
the designer himself.  When you design something
for a group that doesn't include you, it tends to be for people
you consider to be less sophisticated than you, not more sophisticated.That's a problem, because looking down on the user, however benevolently,
seems inevitably to corrupt the designer.
I suspect that very few housing
projects in the US were designed by architects who expected to live
in them.   You can see the same thing
in programming languages.  C, Lisp, and Smalltalk were created for
their own designers to use.  Cobol, Ada, and Java, were created   
for other people to use.If you think you're designing something for idiots, the odds are
that you're not designing something good, even for idiots.
Even if you're designing something for the most sophisticated
users, though, you're still designing for humans.  It's different 
in research.  In math you
don't choose abstractions because they're
easy for humans to understand; you choose whichever make the
proof shorter.  I think this is true for the sciences generally.
Scientific ideas are not meant to be ergonomic.Over in the arts, things are very different.  Design is
all about people.  The human body is a strange
thing, but when you're designing a chair,
that's what you're designing for, and there's no way around it.
All the arts have to pander to the interests and limitations
of humans.   In painting, for example, all other things being
equal a painting with people in it will be more interesting than
one without.  It is not merely an accident of history that
the great paintings of the Renaissance are all full of people.
If they hadn't been, painting as a medium wouldn't have the prestige
that it does.Like it or not, programming languages are also for people,
and I suspect the human brain is just as lumpy and idiosyncratic
as the human body.  Some ideas are easy for people to grasp
and some aren't.  For example, we seem to have a very limited
capacity for dealing with detail.  It's this fact that makes
programing languages a good idea in the first place; if we
could handle the detail, we could just program in machine
language.Remember, too, that languages are not
primarily a form for finished programs, but something that
programs have to be developed in.  Anyone in the arts could
tell you that you might want different mediums for the
two situations.  Marble, for example, is a nice, durable
medium for finished ideas, but a hopelessly inflexible one
for developing new ideas.A program, like a proof,
is a pruned version of a tree that in the past has had
false starts branching off all over it.  So the test of
a language is not simply how clean the finished program looks
in it, but how clean the path to the finished program was.
A design choice that gives you elegant finished programs
may not give you an elegant design process.  For example, 
I've written a few macro-defining macros full of nested
backquotes that look now like little gems, but writing them
took hours of the ugliest trial and error, and frankly, I'm still
not entirely sure they're correct.We often act as if the test of a language were how good
finished programs look in it.
It seems so convincing when you see the same program
written in two languages, and one version is much shorter.
When you approach the problem from the direction of the
arts, you're less likely to depend on this sort of
test.  You don't want to end up with a programming
language like marble.For example, it is a huge win in developing software to
have an interactive toplevel, what in Lisp is called a
read-eval-print loop.  And when you have one this has
real effects on the design of the language.  It would not
work well for a language where you have to declare
variables before using them, for example.  When you're
just typing expressions into the toplevel, you want to be 
able to set x to some value and then start doing things
to x.  You don't want to have to declare the type of x
first.  You may dispute either of the premises, but if
a language has to have a toplevel to be convenient, and
mandatory type declarations are incompatible with a
toplevel, then no language that makes type declarations  
mandatory could be convenient to program in.In practice, to get good design you have to get close, and stay
close, to your users.  You have to calibrate your ideas on actual
users constantly, especially in the beginning.  One of the reasons
Jane Austen's novels are so good is that she read them out loud to
her family.  That's why she never sinks into self-indulgently arty
descriptions of landscapes,
or pretentious philosophizing.  (The philosophy's there, but it's
woven into the story instead of being pasted onto it like a label.)
If you open an average "literary" novel and imagine reading it out loud
to your friends as something you'd written, you'll feel all too
keenly what an imposition that kind of thing is upon the reader.In the software world, this idea is known as Worse is Better.
Actually, there are several ideas mixed together in the concept of
Worse is Better, which is why people are still arguing about
whether worse
is actually better or not.  But one of the main ideas in that
mix is that if you're building something new, you should get a
prototype in front of users as soon as possible.The alternative approach might be called the Hail Mary strategy.
Instead of getting a prototype out quickly and gradually refining
it, you try to create the complete, finished, product in one long
touchdown pass.  As far as I know, this is a
recipe for disaster.  Countless startups destroyed themselves this
way during the Internet bubble.  I've never heard of a case
where it worked.What people outside the software world may not realize is that
Worse is Better is found throughout the arts.
In drawing, for example, the idea was discovered during the
Renaissance.  Now almost every drawing teacher will tell you that
the right way to get an accurate drawing is not to
work your way slowly around the contour of an object, because errors will
accumulate and you'll find at the end that the lines don't meet.
Instead you should draw a few quick lines in roughly the right place,
and then gradually refine this initial sketch.In most fields, prototypes
have traditionally been made out of different materials.
Typefaces to be cut in metal were initially designed  
with a brush on paper.  Statues to be cast in bronze   
were modelled in wax.  Patterns to be embroidered on tapestries
were drawn on paper with ink wash.  Buildings to be
constructed from stone were tested on a smaller scale in wood.What made oil paint so exciting, when it
first became popular in the fifteenth century, was that you
could actually make the finished work from the prototype.
You could make a preliminary drawing if you wanted to, but you
weren't held to it; you could work out all the details, and
even make major changes, as you finished the painting.You can do this in software too.  A prototype doesn't have to
be just a model; you can refine it into the finished product.
I think you should always do this when you can.  It lets you
take advantage of new insights you have along the way.  But
perhaps even more important, it's good for morale.Morale is key in design.  I'm surprised people
don't talk more about it.  One of my first
drawing teachers told me: if you're bored when you're
drawing something, the drawing will look boring.
For example, suppose you have to draw a building, and you
decide to draw each brick individually.  You can do this
if you want, but if you get bored halfway through and start
making the bricks mechanically instead of observing each one,   
the drawing will look worse than if you had merely suggested
the bricks.Building something by gradually refining a prototype is good
for morale because it keeps you engaged.  In software, my  
rule is: always have working code.  If you're writing
something that you'll be able to test in an hour, then you
have the prospect of an immediate reward to motivate you.
The same is true in the arts, and particularly in oil painting.
Most painters start with a blurry sketch and gradually
refine it.
If you work this way, then in principle
you never have to end the day with something that actually
looks unfinished.  Indeed, there is even a saying among
painters: "A painting is never finished, you just stop
working on it."  This idea will be familiar to anyone who
has worked on software.Morale is another reason that it's hard to design something
for an unsophisticated user.   It's hard to stay interested in
something you don't like yourself.  To make something  
good, you have to be thinking, "wow, this is really great,"
not "what a piece of shit; those fools will love it."Design means making things for humans.  But it's not just the
user who's human.  The designer is human too.Notice all this time I've been talking about "the designer."
Design usually has to be under the control of a single person to
be any good.   And yet it seems to be possible for several people
to collaborate on a research project.  This seems to
me one of the most interesting differences between research and
design.There have been famous instances of collaboration in the arts,
but most of them seem to have been cases of molecular bonding rather
than nuclear fusion.  In an opera it's common for one person to
write the libretto and another to write the music.   And during the Renaissance, 
journeymen from northern
Europe were often employed to do the landscapes in the
backgrounds of Italian paintings.  But these aren't true collaborations.
They're more like examples of Robert Frost's
"good fences make good neighbors."  You can stick instances
of good design together, but within each individual project,
one person has to be in control.I'm not saying that good design requires that one person think
of everything.  There's nothing more valuable than the advice
of someone whose judgement you trust.  But after the talking is
done, the decision about what to do has to rest with one person.Why is it that research can be done by collaborators and  
design can't?  This is an interesting question.  I don't 
know the answer.  Perhaps,
if design and research converge, the best research is also
good design, and in fact can't be done by collaborators.
A lot of the most famous scientists seem to have worked alone.
But I don't know enough to say whether there
is a pattern here.  It could be simply that many famous scientists
worked when collaboration was less common.Whatever the story is in the sciences, true collaboration
seems to be vanishingly rare in the arts.  Design by committee is a
synonym for bad design.  Why is that so?  Is there some way to
beat this limitation?I'm inclined to think there isn't-- that good design requires
a dictator.  One reason is that good design has to   
be all of a piece.  Design is not just for humans, but
for individual humans.  If a design represents an idea that  
fits in one person's head, then the idea will fit in the user's
head too.Related:December 2001 (rev. May 2002)

(This article came about in response to some questions on
the LL1 mailing list.  It is now
incorporated in Revenge of the Nerds.)When McCarthy designed Lisp in the late 1950s, it was
a radical departure from existing languages,
the most important of which was Fortran.Lisp embodied nine new ideas:
1. Conditionals.  A conditional is an if-then-else
construct.  We take these for granted now.  They were 
invented
by McCarthy in the course of developing Lisp. 
(Fortran at that time only had a conditional
goto, closely based on the branch instruction in the 
underlying hardware.)  McCarthy, who was on the Algol committee, got
conditionals into Algol, whence they spread to most other
languages.2. A function type. In Lisp, functions are first class 
objects-- they're a data type just like integers, strings,
etc, and have a literal representation, can be stored in variables,
can be passed as arguments, and so on.3. Recursion.  Recursion existed as a mathematical concept
before Lisp of course, but Lisp was the first programming language to support
it.  (It's arguably implicit in making functions first class
objects.)4. A new concept of variables.  In Lisp, all variables
are effectively pointers. Values are what
have types, not variables, and assigning or binding
variables means copying pointers, not what they point to.5. Garbage-collection.6. Programs composed of expressions. Lisp programs are 
trees of expressions, each of which returns a value.  
(In some Lisps expressions
can return multiple values.)  This is in contrast to Fortran
and most succeeding languages, which distinguish between
expressions and statements.It was natural to have this
distinction in Fortran because (not surprisingly in a language
where the input format was punched cards) the language was
line-oriented.  You could not nest statements.  And
so while you needed expressions for math to work, there was
no point in making anything else return a value, because
there could not be anything waiting for it.This limitation
went away with the arrival of block-structured languages,
but by then it was too late. The distinction between
expressions and statements was entrenched.  It spread from 
Fortran into Algol and thence to both their descendants.When a language is made entirely of expressions, you can
compose expressions however you want.  You can say either
(using Arc syntax)(if foo (= x 1) (= x 2))or(= x (if foo 1 2))7. A symbol type.  Symbols differ from strings in that
you can test equality by comparing a pointer.8. A notation for code using trees of symbols.9. The whole language always available.  
There is
no real distinction between read-time, compile-time, and runtime.
You can compile or run code while reading, read or run code
while compiling, and read or compile code at runtime.Running code at read-time lets users reprogram Lisp's syntax;
running code at compile-time is the basis of macros; compiling
at runtime is the basis of Lisp's use as an extension
language in programs like Emacs; and reading at runtime
enables programs to communicate using s-expressions, an
idea recently reinvented as XML.
When Lisp was first invented, all these ideas were far
removed from ordinary programming practice, which was
dictated largely by the hardware available in the late 1950s.Over time, the default language, embodied
in a succession of popular languages, has
gradually evolved toward Lisp.  1-5 are now widespread.
6 is starting to appear in the mainstream.
Python has a form of 7, though there doesn't seem to be
any syntax for it.  
8, which (with 9) is what makes Lisp macros
possible, is so far still unique to Lisp,
perhaps because (a) it requires those parens, or something 
just as bad, and (b) if you add that final increment of power, 
you can no 
longer claim to have invented a new language, but only
to have designed a new dialect of Lisp ; -)Though useful to present-day programmers, it's
strange to describe Lisp in terms of its
variation from the random expedients other languages
adopted.  That was not, probably, how McCarthy
thought of it.  Lisp wasn't designed to fix the mistakes
in Fortran; it came about more as the byproduct of an
attempt to axiomatize computation.December 2014If the world were static, we could have monotonically increasing
confidence in our beliefs.  The more (and more varied) experience
a belief survived, the less likely it would be false.  Most people
implicitly believe something like this about their opinions.  And
they're justified in doing so with opinions about things that don't
change much, like human nature.  But you can't trust your opinions
in the same way about things that change, which could include
practically everything else.When experts are wrong, it's often because they're experts on an
earlier version of the world.Is it possible to avoid that?  Can you protect yourself against
obsolete beliefs?  To some extent, yes. I spent almost a decade
investing in early stage startups, and curiously enough protecting
yourself against obsolete beliefs is exactly what you have to do
to succeed as a startup investor.  Most really good startup ideas
look like bad ideas at first, and many of those look bad specifically
because some change in the world just switched them from bad to
good.  I spent a lot of time learning to recognize such ideas, and
the techniques I used may be applicable to ideas in general.The first step is to have an explicit belief in change.  People who
fall victim to a monotonically increasing confidence in their
opinions are implicitly concluding the world is static.  If you
consciously remind yourself it isn't, you start to look for change.Where should one look for it?  Beyond the moderately useful
generalization that human nature doesn't change much, the unfortunate
fact is that change is hard to predict.  This is largely a tautology
but worth remembering all the same: change that matters usually
comes from an unforeseen quarter.So I don't even try to predict it.  When I get asked in interviews
to predict the future, I always have to struggle to come up with
something plausible-sounding on the fly, like a student who hasn't
prepared for an exam.
[1]
But it's not out of laziness that I haven't
prepared.  It seems to me that beliefs about the future are so
rarely correct that they usually aren't worth the extra rigidity
they impose, and that the best strategy is simply to be aggressively
open-minded.  Instead of trying to point yourself in the right
direction, admit you have no idea what the right direction is, and
try instead to be super sensitive to the winds of change.It's ok to have working hypotheses, even though they may constrain
you a bit, because they also motivate you.  It's exciting to chase
things and exciting to try to guess answers.  But you have to be
disciplined about not letting your hypotheses harden into anything
more.
[2]I believe this passive m.o. works not just for evaluating new ideas
but also for having them.  The way to come up with new ideas is not
to try explicitly to, but to try to solve problems and simply not
discount weird hunches you have in the process.The winds of change originate in the unconscious minds of domain
experts.  If you're sufficiently expert in a field, any weird idea
or apparently irrelevant question that occurs to you is ipso facto
worth exploring. 
[3]
 Within Y Combinator, when an idea is described
as crazy, it's a compliment—in fact, on average probably a
higher compliment than when an idea is described as good.Startup investors have extraordinary incentives for correcting
obsolete beliefs.  If they can realize before other investors that
some apparently unpromising startup isn't, they can make a huge
amount of money.  But the incentives are more than just financial.
Investors' opinions are explicitly tested: startups come to them
and they have to say yes or no, and then, fairly quickly, they learn
whether they guessed right.  The investors who say no to a Google
(and there were several) will remember it for the rest of their
lives.Anyone who must in some sense bet on ideas rather than merely
commenting on them has similar incentives.  Which means anyone who
wants such incentives can have them, by turning their comments into
bets: if you write about a topic in some fairly durable and public
form, you'll find you worry much more about getting things right
than most people would in a casual conversation.
[4]Another trick I've found to protect myself against obsolete beliefs
is to focus initially on people rather than ideas. Though the nature
of future discoveries is hard to predict, I've found I can predict
quite well what sort of people will make them.  Good new ideas come
from earnest, energetic, independent-minded people.Betting on people over ideas saved me countless times as an investor.
We thought Airbnb was a bad idea, for example. But we could tell
the founders were earnest, energetic, and independent-minded.
(Indeed, almost pathologically so.)  So we suspended disbelief and
funded them.This too seems a technique that should be generally applicable.
Surround yourself with the sort of people new ideas come from.  If
you want to notice quickly when your beliefs become obsolete, you
can't do better than to be friends with the people whose discoveries
will make them so.It's hard enough already not to become the prisoner of your own
expertise, but it will only get harder, because change is accelerating.
That's not a recent trend; change has been accelerating since the
paleolithic era.  Ideas beget ideas.  I don't expect that to change.
But I could be wrong.
Notes[1]
My usual trick is to talk about aspects of the present that
most people haven't noticed yet.[2]
Especially if they become well enough known that people start
to identify them with you.  You have to be extra skeptical about
things you want to believe, and once a hypothesis starts to be
identified with you, it will almost certainly start to be in that
category.[3]
In practice "sufficiently expert" doesn't require one to be
recognized as an expert—which is a trailing indicator in any
case.  In many fields a year of focused work plus caring a lot would
be enough.[4]
Though they are public and persist indefinitely, comments on
e.g. forums and places like Twitter seem empirically to work like
casual conversation.  The threshold may be whether what you write
has a title.
Thanks to Sam Altman, Patrick Collison, and Robert Morris
for reading drafts of this.

Want to start a startup?  Get funded by
Y Combinator.




October 2010

(I wrote this for Forbes, who asked me to write something
about the qualities we look for in founders.  In print they had to cut
the last item because they didn't have room.)1. DeterminationThis has turned out to be the most important quality in startup
founders.  We thought when we started Y Combinator that the most
important quality would be intelligence.  That's the myth in the
Valley. And certainly you don't want founders to be stupid.  But
as long as you're over a certain threshold of intelligence, what
matters most is determination.  You're going to hit a lot of
obstacles.  You can't be the sort of person who gets demoralized
easily.Bill Clerico and Rich Aberman of WePay 
are a good example.  They're
doing a finance startup, which means endless negotiations with big,
bureaucratic companies.  When you're starting a startup that depends
on deals with big companies to exist, it often feels like they're
trying to ignore you out of existence.  But when Bill Clerico starts
calling you, you may as well do what he asks, because he is not
going away.
2. FlexibilityYou do not however want the sort of determination implied by phrases
like "don't give up on your dreams."  The world of startups is so
unpredictable that you need to be able to modify your dreams on the
fly.  The best metaphor I've found for the combination of determination
and flexibility you need is a running back.  
He's determined to get
downfield, but at any given moment he may need to go sideways or
even backwards to get there.The current record holder for flexibility may be Daniel Gross of
Greplin.  He applied to YC with 
some bad ecommerce idea.  We told
him we'd fund him if he did something else.  He thought for a second,
and said ok.  He then went through two more ideas before settling
on Greplin.  He'd only been working on it for a couple days when
he presented to investors at Demo Day, but he got a lot of interest.
He always seems to land on his feet.
3. ImaginationIntelligence does matter a lot of course.  It seems like the type
that matters most is imagination.  It's not so important to be able
to solve predefined problems quickly as to be able to come up with
surprising new ideas.  In the startup world, most good ideas 
seem
bad initially.  If they were obviously good, someone would already
be doing them.  So you need the kind of intelligence that produces
ideas with just the right level of craziness.Airbnb is that kind of idea.  
In fact, when we funded Airbnb, we
thought it was too crazy.  We couldn't believe large numbers of
people would want to stay in other people's places.  We funded them
because we liked the founders so much.  As soon as we heard they'd
been supporting themselves by selling Obama and McCain branded
breakfast cereal, they were in.  And it turned out the idea was on
the right side of crazy after all.
4. NaughtinessThough the most successful founders are usually good people, they
tend to have a piratical gleam in their eye.  They're not Goody
Two-Shoes type good.  Morally, they care about getting the big
questions right, but not about observing proprieties.  That's why
I'd use the word naughty rather than evil.  They delight in 
breaking
rules, but not rules that matter.  This quality may be redundant
though; it may be implied by imagination.Sam Altman of Loopt 
is one of the most successful alumni, so we
asked him what question we could put on the Y Combinator application
that would help us discover more people like him.  He said to ask
about a time when they'd hacked something to their advantage—hacked in the sense of beating the system, not breaking into
computers.  It has become one of the questions we pay most attention
to when judging applications.
5. FriendshipEmpirically it seems to be hard to start a startup with just 
one
founder.  Most of the big successes have two or three.  And the
relationship between the founders has to be strong.  They must
genuinely like one another, and work well together.  Startups do
to the relationship between the founders what a dog does to a sock:
if it can be pulled apart, it will be.Emmett Shear and Justin Kan of Justin.tv 
are a good example of close
friends who work well together.  They've known each other since
second grade.  They can practically read one another's minds.  I'm
sure they argue, like all founders, but I have never once sensed
any unresolved tension between them.Thanks to Jessica Livingston and Chris Steiner for reading drafts of this.

April 2009I usually avoid politics, but since we now seem to have an administration that's open to suggestions, I'm going to risk making one.  The single biggest thing the government could do to increase the number of startups in this country is a policy that would cost nothing: establish a new class of visa for startup founders.The biggest constraint on the number of new startups that get created in the US is not tax policy or employment law or even Sarbanes-Oxley.  It's that we won't let the people who want to start them into the country.Letting just 10,000 startup founders into the country each year could have a visible effect on the economy.  If we assume 4 people per startup, which is probably an overestimate, that's 2500 new companies.  Each year.  They wouldn't all grow as big as Google, but out of 2500 some would come close.By definition these 10,000 founders wouldn't be taking jobs from Americans: it could be part of the terms of the visa that they couldn't work for existing companies, only new ones they'd founded.  In fact they'd cause there to be 
more jobs for Americans, because the companies they started would hire more employees as they grew.The tricky part might seem to be how one defined a startup. But that could be solved quite easily: let the market decide.  Startup investors work hard to find the best startups.  The government could not do better than to piggyback on their expertise, and use investment by recognized startup investors as the test of whether a company was a real startup.How would the government decide who's a startup investor?  The same way they decide what counts as a university for student visas. We'll establish our own accreditation procedure. We know who one another are.10,000 people is a drop in the bucket by immigration standards, but would represent a huge increase in the pool of startup founders.  I think this would have such a visible effect on the economy that it would make the legislator who introduced the bill famous.  The only way to know for sure would be to try it, and that would cost practically nothing.
Thanks to Trevor Blackwell, Paul Buchheit, Jeff Clavier, David Hornik, Jessica Livingston, Greg Mcadoo, Aydin Senkut, and Fred Wilson for reading drafts of this.Related:May 2004When people care enough about something to do it well, those who
do it best tend to be far better than everyone else.  There's a
huge gap between Leonardo and second-rate contemporaries like
Borgognone.  You see the same gap between Raymond Chandler and the
average writer of detective novels.  A top-ranked professional chess
player could play ten thousand games against an ordinary club player
without losing once.Like chess or painting or writing novels, making money is a very
specialized skill.   But for some reason we treat this skill
differently.  No one complains when a few people surpass all the
rest at playing chess or writing novels, but when a few people make
more money than the rest, we get editorials saying this is wrong.Why?  The pattern of variation seems no different than for any other
skill.  What causes people to react so strongly when the skill is
making money?I think there are three reasons we treat making money as different:
the misleading model of wealth we learn as children; the disreputable
way in which, till recently, most fortunes were accumulated; and
the worry that great variations in income are somehow bad for
society.  As far as I can tell, the first is mistaken, the second
outdated, and the third empirically false.  Could it be that, in a
modern democracy, variation in income is actually a sign of health?The Daddy Model of WealthWhen I was five I thought electricity was created by electric
sockets.  I didn't realize there were power plants out there
generating it.  Likewise, it doesn't occur to most kids that wealth
is something that has to be generated.  It seems to be something
that flows from parents.Because of the circumstances in which they encounter it, children
tend to misunderstand wealth.  They confuse it with money.  They
think that there is a fixed amount of it.  And they think of it as
something that's distributed by authorities (and so should be
distributed equally), rather than something that has to be created
(and might be created unequally).In fact, wealth is not money.  Money is just a convenient way of
trading one form of wealth for another.  Wealth is the underlying
stuff—the goods and services we buy.  When you travel to a
rich or poor country, you don't have to look at people's bank
accounts to tell which kind you're in.  You can see
wealth—in buildings and streets, in the clothes and the health
of the people.Where does wealth come from?  People make it.  This was easier to
grasp when most people lived on farms, and made many of the things
they wanted with their own hands.  Then you could see in the house,
the herds, and the granary the wealth that each family created.  It
was obvious then too that the wealth of the world was not a fixed
quantity that had to be shared out, like slices of a pie.  If you
wanted more wealth, you could make it.This is just as true today, though few of us create wealth directly
for ourselves (except for a few vestigial domestic tasks).  Mostly
we create wealth for other people in exchange for money, which we
then trade for the forms of wealth we want. 
[1]Because kids are unable to create wealth, whatever they have has
to be given to them.  And when wealth is something you're given,
then of course it seems that it should be distributed equally.
[2]
As in most families it is.  The kids see to that.  "Unfair," they
cry, when one sibling gets more than another.In the real world, you can't keep living off your parents.  If you
want something, you either have to make it, or do something of
equivalent value for someone else, in order to get them to give you
enough money to buy it.  In the real world, wealth is (except for
a few specialists like thieves and speculators) something you have
to create, not something that's distributed by Daddy.  And since
the ability and desire to create it vary from person to person,
it's not made equally.You get paid by doing or making something people want, and those
who make more money are often simply better at doing what people
want.  Top actors make a lot more money than B-list actors.  The
B-list actors might be almost as charismatic, but when people go
to the theater and look at the list of movies playing, they want
that extra oomph that the big stars have.Doing what people want is not the only way to get money, of course.
You could also rob banks, or solicit bribes, or establish a monopoly.
Such tricks account for some variation in wealth, and indeed for
some of the biggest individual fortunes, but they are not the root
cause of variation in income.  The root cause of variation in income,
as Occam's Razor implies, is the same as the root cause of variation
in every other human skill.In the United States, the CEO of a large public company makes about
100 times as much as the average person. 
[3]
Basketball players
make about 128 times as much, and baseball players 72 times as much.
Editorials quote this kind of statistic with horror.  But I have
no trouble imagining that one person could be 100 times as productive
as another.  In ancient Rome the price of slaves varied by
a factor of 50 depending on their skills. 
[4]
And that's without
considering motivation, or the extra leverage in productivity that
you can get from modern technology.Editorials about athletes' or CEOs' salaries remind me of early
Christian writers, arguing from first principles about whether the
Earth was round, when they could just walk outside and check.
[5]
How much someone's work is worth is not a policy question.  It's
something the market already determines."Are they really worth 100 of us?" editorialists ask.  Depends on
what you mean by worth.  If you mean worth in the sense of what
people will pay for their skills, the answer is yes, apparently.A few CEOs' incomes reflect some kind of wrongdoing.  But are there
not others whose incomes really do reflect the wealth they generate?
Steve Jobs saved a company that was in a terminal decline.  And not
merely in the way a turnaround specialist does, by cutting costs;
he had to decide what Apple's next products should be.  Few others
could have done it.  And regardless of the case with CEOs, it's
hard to see how anyone could argue that the salaries of professional
basketball players don't reflect supply and demand.It may seem unlikely in principle that one individual could really
generate so much more wealth than another.  The key to this mystery
is to revisit that question, are they really worth 100 of us?
Would a basketball team trade one of their players for 100
random people?  What would Apple's next product look like if you
replaced Steve Jobs with a committee of 100 random people? 
[6]
These
things don't scale linearly.  Perhaps the CEO or the professional
athlete has only ten times (whatever that means) the skill and
determination of an ordinary person.  But it makes all the difference
that it's concentrated in one individual.When we say that one kind of work is overpaid and another underpaid,
what are we really saying?  In a free market, prices are determined
by what buyers want.  People like baseball more than  poetry, so
baseball players make more than poets.  To say that a certain kind
of work is underpaid is thus identical with saying that people want
the wrong things.Well, of course people want the wrong things.  It seems odd to be
surprised by that.  And it seems even odder to say that it's
unjust that certain kinds of work are underpaid. 
[7]
Then
you're saying that it's unjust that people want the wrong things.
It's  lamentable that people prefer reality TV and corndogs to
Shakespeare and steamed vegetables, but unjust?  That seems like
saying that blue is heavy, or that up is circular.The appearance of the word "unjust" here is the unmistakable spectral
signature of the Daddy Model.  Why else would this idea occur in
this odd context?  Whereas if the speaker were still operating on
the Daddy Model, and saw wealth as something that flowed from a
common source and had to be shared out, rather than something
generated by doing what other people wanted, this is exactly what
you'd get on noticing that some people made much more than others.When we talk about "unequal distribution of income," we should
also ask, where does that income come from?
[8]
Who made the wealth
it represents?  Because to the extent that income varies simply
according to how much wealth people create, the distribution may
be unequal, but it's hardly unjust.Stealing ItThe second reason we tend to find great disparities of wealth
alarming is that for most of human history the usual way to accumulate
a fortune was to steal it: in pastoral societies by cattle raiding;
in agricultural societies by appropriating others' estates in times
of war, and taxing them in times of peace.In conflicts, those on the winning side would receive the estates
confiscated from the losers.  In England in the 1060s, when William
the Conqueror distributed the estates of the defeated Anglo-Saxon
nobles to his followers, the conflict was military.  By the 1530s,
when Henry VIII distributed the estates of the monasteries to his
followers, it was mostly political. 
[9]
But the principle was the
same.  Indeed, the same principle is at work now in Zimbabwe.In more organized societies, like China, the ruler and his officials
used taxation instead of confiscation.  But here too we see the
same principle: the way to get rich was not to create wealth, but
to serve a ruler powerful enough to appropriate it.This started to change in Europe with the rise of the middle class.
Now we think of the middle class as people who are neither rich nor
poor, but originally they were a distinct group.  In a feudal
society, there are just two classes: a warrior aristocracy, and the
serfs who work their estates.  The middle class were a new, third
group who lived in towns and supported themselves by manufacturing
and trade.Starting in the tenth and eleventh centuries, petty nobles and
former serfs banded together in towns that gradually became powerful
enough to ignore the local feudal lords. 
[10]
Like serfs, the middle
class made a living largely by creating wealth.  (In port cities
like Genoa and Pisa, they also engaged in piracy.) But unlike serfs
they had an incentive to create a lot of it.  Any wealth a serf
created belonged to his master.  There was not much point in making
more than you could hide.  Whereas the independence of the townsmen
allowed them to keep whatever wealth they created.Once it became possible to get rich by creating wealth, society as
a whole started to get richer very rapidly.  Nearly everything we
have was created by the middle class.  Indeed, the other two classes
have effectively disappeared in industrial societies, and their
names been given to either end of the middle class.  (In the original
sense of the word, Bill Gates is middle class.)But it was not till the Industrial Revolution that wealth creation
definitively replaced corruption as the best way to get rich.  In
England, at least, corruption only became unfashionable (and in
fact only started to be called "corruption") when there started to
be other, faster ways to get rich.Seventeenth-century England was much like the third world today,
in that government office was a recognized route to wealth.  The
great fortunes of that time still derived more from what we would
now call corruption than from commerce. 
[11]
By the nineteenth
century that had changed.  There continued to be bribes, as there
still are everywhere, but politics had by then been left to men who
were driven more by vanity than greed.  Technology had made it
possible to create wealth faster than you could steal it.  The
prototypical rich man of the nineteenth century was not a courtier
but an industrialist.With the rise of the middle class, wealth stopped being a zero-sum
game.  Jobs and Wozniak didn't have to make us poor to make themselves
rich.  Quite the opposite: they created things that made our lives
materially richer.  They had to, or we wouldn't have paid for them.But since for most of the world's history the main route to wealth
was to steal it, we tend to be suspicious of rich people.  Idealistic
undergraduates find their unconsciously preserved child's model of
wealth confirmed by eminent writers of the past.  It is a case of
the mistaken meeting the outdated."Behind every great fortune, there is a crime," Balzac wrote.  Except
he didn't.  What he actually said was that a great fortune with no
apparent cause was probably due to a crime well enough executed
that it had been forgotten.  If we were talking about Europe in
1000, or most of the third world today, the standard misquotation
would be spot on.  But Balzac lived in nineteenth-century France,
where the Industrial Revolution was well advanced.  He knew you
could make a fortune without stealing it.  After all, he did himself,
as a popular novelist.
[12]Only a few countries (by no coincidence, the richest ones) have
reached this stage.  In most, corruption still has the upper hand.
In most, the fastest way to get wealth is by stealing it.  And so
when we see increasing differences in income in a rich country,
there is a tendency to worry that it's sliding back toward becoming
another Venezuela.  I think the opposite is happening. I think
you're seeing a country a full step ahead of Venezuela.The Lever of TechnologyWill technology increase the gap between rich and poor?  It will
certainly increase the gap between the productive and the unproductive.
That's the whole point of technology.   With a tractor an energetic
farmer could plow six times as much land in a day as he could with
a team of horses.  But only if he mastered a new kind of farming.I've seen the lever of technology grow visibly in my own time.  In
high school I made money by mowing lawns and scooping ice cream at
Baskin-Robbins.  This was the only kind of work available at the
time.  Now high school kids could write software or design web
sites.  But only some of them will; the rest will still be scooping
ice cream.I remember very vividly when in 1985 improved technology made it
possible for me to buy a computer of my own.  Within months I was
using it to make money as a freelance programmer.  A few years
before, I couldn't have done this.  A few years before, there was
no such thing as a freelance programmer.  But Apple created
wealth, in the form of powerful, inexpensive computers, and programmers
immediately set to work using it to create more.As this example suggests, the rate at which technology increases
our productive capacity is probably exponential, rather than linear.
So we should expect to see ever-increasing variation in individual
productivity as time goes on.   Will that increase the gap between
rich and the poor?  Depends which gap you mean.Technology should increase the gap in income, but it seems to
decrease other gaps.  A hundred years ago, the rich led a different
kind of life from ordinary people.  They lived in houses
full of servants, wore elaborately uncomfortable clothes, and
travelled about in carriages drawn by teams of horses which themselves
required their own houses and servants.  Now, thanks to technology,
the rich live more like the average person.Cars are a good example of why.  It's possible to buy expensive,
handmade cars that cost hundreds of thousands of dollars.  But there
is not much point.  Companies make more money by building a large
number of ordinary cars than a small number of expensive ones.  So
a company making a mass-produced car can afford to spend a lot more
on its design.  If you buy a custom-made car, something will always
be breaking.  The only point of buying one now is to advertise that
you can.Or consider watches.  Fifty years ago, by spending a lot of money
on a watch you could get better performance.  When watches had
mechanical movements, expensive watches kept better time.  Not any
more.  Since the invention of the quartz movement, an ordinary Timex
is more accurate than a Patek Philippe costing hundreds of thousands
of dollars.
[13]
Indeed, as with expensive cars, if you're determined
to spend a lot of money on a watch, you have to put up with some
inconvenience to do it: as well as keeping worse time, mechanical
watches have to be wound.The only thing technology can't cheapen is brand.  Which is precisely
why we hear ever more about it.  Brand is the residue left as the
substantive differences between rich and poor evaporate.  But what
label you have on your stuff is a much smaller matter than having
it versus not having it.  In 1900, if you kept a carriage, no one
asked what year or brand it was.  If you had one, you were rich.
And if you weren't rich, you took the omnibus or walked.  Now even
the poorest Americans drive cars, and it is only because we're so
well trained by advertising that we can even recognize the especially
expensive ones.
[14]The same pattern has played out in industry after industry.  If
there is enough demand for something, technology will make it cheap
enough to sell in large volumes, and the mass-produced versions
will be, if not better, at least more convenient.
[15]
And there
is nothing the rich like more than convenience.  The rich people I
know drive the same cars, wear the same clothes, have the same kind
of furniture, and eat the same foods as my other friends.  Their
houses are in different neighborhoods, or if in the same neighborhood
are different sizes, but within them life is similar.  The houses
are made using the same construction techniques and contain much
the same objects.  It's inconvenient to do something expensive and
custom.The rich spend their time more like everyone else too.  Bertie
Wooster seems long gone.  Now, most people who are rich enough not
to work do anyway.  It's not just social pressure that makes them;
idleness is lonely and demoralizing.Nor do we have the social distinctions there were a hundred years
ago.   The novels and etiquette manuals of that period read now
like descriptions of some strange tribal society.  "With respect
to the continuance of friendships..." hints Mrs. Beeton's Book
of Household Management (1880), "it may be found necessary, in
some cases, for a mistress to relinquish, on assuming the responsibility
of a household, many of those commenced in the earlier part of her
life." A woman who married a rich man was expected to drop friends
who didn't.  You'd seem a barbarian if you behaved that way today.
You'd also have a very boring life.  People still tend to segregate
themselves somewhat, but much more on the basis of education than
wealth.
[16]Materially and socially, technology seems to be decreasing the gap
between the rich and the poor, not increasing it.  If Lenin walked
around the offices of a company like Yahoo or Intel or Cisco, he'd
think communism had won.  Everyone would be wearing the same clothes,
have the same kind of office (or rather, cubicle) with the same
furnishings, and address one another by their first names instead
of by honorifics.  Everything would seem exactly as he'd predicted,
until he looked at their bank accounts.  Oops.Is it a problem if technology increases that gap?  It doesn't seem
to be so far.  As it increases the gap in income, it seems to
decrease most other gaps.Alternative to an AxiomOne often hears a policy criticized on the grounds that it would
increase the income gap between rich and poor.  As if it were an
axiom that this would be bad.  It might be true that increased
variation in income would be bad, but I don't see how we can say
it's axiomatic.Indeed, it may even be false, in industrial democracies.  In a
society of serfs and warlords, certainly, variation in income is a
sign of an underlying problem.  But serfdom is not the only cause
of variation in income.  A 747 pilot doesn't make 40 times as much
as a checkout clerk because he is a warlord who somehow holds her
in thrall.  His skills are simply much more valuable.I'd like to propose an alternative idea: that in a modern society,
increasing variation in income is a sign of health.  Technology
seems to increase the variation in productivity at faster than
linear rates.  If we don't see corresponding variation in income,
there are three possible explanations: (a) that technical innovation
has stopped, (b) that the people who would create the most wealth
aren't doing it, or (c) that they aren't getting paid for it.I think we can safely say that (a) and (b) would be bad.  If you
disagree, try living for a year using only the resources available
to the average Frankish nobleman in 800, and report back to us.
(I'll be generous and not send you back to the stone age.)The only option, if you're going to have an increasingly prosperous
society without increasing variation in income, seems to be (c),
that people will create a lot of wealth without being paid for it.
That Jobs and Wozniak, for example, will cheerfully work 20-hour
days to produce the Apple computer for a society that allows them,
after taxes, to keep just enough of their income to match what they
would have made working 9 to 5 at a big company.Will people create wealth if they can't get paid for it?  Only if
it's fun.  People will write operating systems for free.  But they
won't install them, or take support calls, or train customers to
use them.  And at least 90% of the work that even the highest tech
companies do is of this second, unedifying kind.All the unfun kinds of wealth creation slow dramatically in a society
that confiscates private fortunes.  We can confirm this empirically.
Suppose you hear a strange noise that you think may be due to a
nearby fan.  You turn the fan off, and the noise stops.  You turn
the fan back on, and the noise starts again.  Off, quiet.  On,
noise.  In the absence of other information, it would seem the noise
is caused by the fan.At various times and places in history, whether you could accumulate
a fortune by creating wealth has been turned on and off.  Northern
Italy in 800, off (warlords would steal it).  Northern Italy in
1100, on.  Central France in 1100, off (still feudal).  England in
1800, on.  England in 1974, off (98% tax on investment income).
United States in 1974, on.  We've even had a twin study: West
Germany, on;  East Germany, off.  In every case, the creation of
wealth seems to appear and disappear like the noise of a fan as you
switch on and off the prospect of keeping it.There is some momentum involved.  It probably takes at least a
generation to turn people into East Germans (luckily for England).
But if it were merely a fan we were studying, without all the extra
baggage that comes from the controversial topic of wealth, no one
would have any doubt that the fan was causing the noise.If you suppress variations in income, whether by stealing private
fortunes, as feudal rulers used to do, or by taxing them away, as
some modern governments have done, the result always seems to be
the same.    Society as a whole ends up poorer.If I had a choice of living in a society where I was materially
much better off than I am now, but was among the poorest, or in one
where I was the richest, but much worse off than I am now, I'd take
the first option.  If I had children, it would arguably be immoral
not to.  It's absolute poverty you want to avoid, not relative
poverty.  If, as the evidence so far implies, you have to have one
or the other in your society, take relative poverty.You need rich people in your society not so much because in spending
their money they create jobs, but because of what they have to do
to get rich.  I'm not talking about the trickle-down effect
here.  I'm not saying that if you let Henry Ford get rich, he'll
hire you as a waiter at his next party.  I'm saying that he'll make
you a tractor to replace your horse.Notes[1]
Part of the reason this subject is so contentious is that some
of those most vocal on the subject of wealth—university
students, heirs, professors, politicians, and journalists—have
the least experience creating it.  (This phenomenon will be familiar
to anyone who has overheard conversations about sports in a bar.)Students are mostly still on the parental dole, and have not stopped
to think about where that money comes from.  Heirs will be on the
parental dole for life.  Professors and politicians live within
socialist eddies of the economy, at one remove from the creation
of wealth, and are paid a flat rate regardless of how hard they
work.
The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.
  And journalists as part of their professional code segregate
themselves from the revenue-collecting half of the businesses they
work for (the ad sales department).  Many of these people never
come face to face with the fact that the money they receive represents
wealth—wealth that, except in the case of journalists, someone
else created earlier.  They live in a world in which income is
doled out by a central authority according to some abstract notion
of fairness (or randomly, in the case of heirs), rather than given
by other people in return for something they wanted, so it may seem
to them unfair that things don't work the same in the rest of the
economy.(Some professors do create a great deal of wealth for
society.  But the money they're paid isn't a quid pro quo.
It's more in the nature of an investment.)[2]
When one reads about the origins of the Fabian Society, it
sounds like something cooked up by the high-minded Edwardian
child-heroes of Edith Nesbit's The Wouldbegoods.[3]
According to a study by the Corporate Library, the median total
compensation, including salary, bonus, stock grants, and the exercise
of stock options, of S&P 500 CEOs in 2002 was $3.65 million.
According to Sports Illustrated, the average NBA player's
salary during the 2002-03 season was $4.54 million, and the average
major league baseball player's salary at the start of the 2003
season was $2.56 million.  According to the Bureau of Labor
Statistics, the mean annual wage in the US in 2002 was $35,560.[4]
In the early empire the price of an ordinary adult slave seems
to have been about 2,000 sestertii (e.g. Horace, Sat. ii.7.43).
A servant girl cost 600 (Martial vi.66), while Columella (iii.3.8)
says that a skilled vine-dresser was worth 8,000.  A doctor, P.
Decimus Eros Merula, paid 50,000 sestertii for his freedom (Dessau,
Inscriptiones 7812).  Seneca (Ep. xxvii.7) reports
that one Calvisius Sabinus paid 100,000 sestertii apiece for slaves
learned in the Greek classics.  Pliny (Hist. Nat. vii.39)
says that the highest price paid for a slave up to his time was
700,000 sestertii, for the linguist (and presumably teacher) Daphnis,
but that this had since been exceeded by actors buying their own
freedom.Classical Athens saw a similar variation in prices.  An ordinary
laborer was worth about 125 to 150 drachmae.  Xenophon (Mem.
ii.5) mentions prices ranging from 50 to 6,000 drachmae (for the
manager of a silver mine).For more on the economics of ancient slavery see:Jones, A. H. M., "Slavery in the Ancient World," Economic History
Review, 2:9 (1956), 185-199, reprinted in Finley, M. I. (ed.),
Slavery in Classical Antiquity, Heffer, 1964.[5]
Eratosthenes (276—195 BC) used shadow lengths in different
cities to estimate the Earth's circumference.  He was off by only
about 2%.[6]
No, and Windows, respectively.[7]
One of the biggest divergences between the Daddy Model and
reality is the valuation of hard work.  In the Daddy Model, hard
work is in itself deserving.  In reality, wealth is measured by
what one delivers, not how much effort it costs.  If I paint someone's
house, the owner shouldn't pay me extra for doing it with a toothbrush.It will seem to someone still implicitly operating on the Daddy
Model that it is unfair when someone works hard and doesn't get
paid much.  To help clarify the matter, get rid of everyone else
and put our worker on a desert island, hunting and gathering fruit.
If he's bad at it he'll work very hard and not end up with much
food.  Is this unfair?  Who is being unfair to him?[8]
Part of the reason for the tenacity of the Daddy Model may be
the dual meaning of "distribution." When economists talk about
"distribution of income," they mean statistical distribution.  But
when you use the phrase frequently, you can't help associating it
with the other sense of the word (as in e.g. "distribution of alms"),
and thereby subconsciously seeing wealth as something that flows
from some central tap.  The word "regressive" as applied to tax
rates has a similar effect, at least on me; how can anything
regressive be good?[9]
"From the beginning of the reign Thomas Lord Roos was an assiduous
courtier of the young Henry VIII and was soon to reap the rewards.
In 1525 he was made a Knight of the Garter and given the Earldom
of Rutland.  In the thirties his support of the breach with Rome,
his zeal in crushing the Pilgrimage of Grace, and his readiness to
vote the death-penalty in the succession of spectacular treason
trials that punctuated Henry's erratic matrimonial progress made
him an obvious candidate for grants of monastic property."Stone, Lawrence, Family and Fortune: Studies in Aristocratic
Finance in the Sixteenth and Seventeenth Centuries, Oxford
University Press, 1973, p. 166.[10]
There is archaeological evidence for large settlements earlier,
but it's hard to say what was happening in them.Hodges, Richard and David Whitehouse, Mohammed, Charlemagne and
the Origins of Europe, Cornell University Press, 1983.[11]
William Cecil and his son Robert were each in turn the most
powerful minister of the crown, and both used their position to
amass fortunes among the largest of their times.  Robert in particular
took bribery to the point of treason.  "As Secretary of State and
the leading advisor to King James on foreign policy, [he] was a
special recipient of favour, being offered large bribes by the Dutch
not to make peace with Spain, and large bribes by Spain to make
peace." (Stone, op. cit., p. 17.)[12]
Though Balzac made a lot of money from writing, he was notoriously
improvident and was troubled by debts all his life.[13]
A Timex will gain or lose about.5 seconds per day.  The most
accurate mechanical watch, the Patek Philippe 10 Day Tourbillon,
is rated at -1.5 to +2 seconds.  Its retail price is about $220,000.[14]
If asked to choose which was more expensive, a well-preserved
1989 Lincoln Town Car ten-passenger limousine ($5,000) or a 2004
Mercedes S600 sedan ($122,000), the average Edwardian might well
guess wrong.[15]
To say anything meaningful about income trends, you have to
talk about real income, or income as measured in what it can buy.
But the usual way of calculating real income ignores much of the
growth in wealth over time, because it depends on a consumer price
index created by bolting end to end a series of numbers that are
only locally accurate, and that don't include the prices of new
inventions until they become so common that their prices stabilize.So while we might think it was very much better to live in a world
with antibiotics or air travel or an electric power grid than
without, real income statistics calculated in the usual way will
prove to us that we are only slightly richer for having these things.Another approach would be to ask, if you were going back to the
year x in a time machine, how much would you have to spend on trade
goods to make your fortune?  For example, if you were going back
to 1970 it would certainly be less than $500, because the processing
power you can get for $500 today would have been worth at least
$150 million in 1970.  The function goes asymptotic fairly quickly,
because for times over a hundred years or so you could get all you
needed in present-day trash.  In 1800 an empty plastic drink bottle
with a screw top would have seemed a miracle of workmanship.[16]
Some will say this amounts to the same thing, because the rich
have better opportunities for education.  That's a valid point.  It
is still possible, to a degree, to buy your kids' way into top
colleges by sending them to private schools that in effect hack the
college admissions process.According to a 2002 report by the National Center for Education
Statistics, about 1.7% of American kids attend private, non-sectarian
schools.  At Princeton, 36% of the class of 2007 came from such
schools.  (Interestingly, the number at Harvard is significantly
lower, about 28%.)  Obviously this is a huge loophole.  It does at
least seem to be closing, not widening.Perhaps the designers of admissions processes should take a lesson
from the example of computer security, and instead of just assuming
that their system can't be hacked, measure the degree to which it
is.April 2004To the popular press, "hacker" means someone who breaks
into computers.  Among programmers it means a good programmer.
But the two meanings are connected.  To programmers,
"hacker" connotes mastery in the most literal sense: someone
who can make a computer do what he wants—whether the computer
wants to or not.To add to the confusion, the noun "hack" also has two senses.  It can
be either a compliment or an insult.  It's called a hack when
you do something in an ugly way.  But when you do something
so clever that you somehow beat the system, that's also
called a hack.  The word is used more often in the former than
the latter sense, probably because ugly solutions are more
common than brilliant ones.Believe it or not, the two senses of "hack" are also
connected.  Ugly and imaginative solutions have something in
common: they both break the rules.  And there is a gradual
continuum between rule breaking that's merely ugly (using
duct tape to attach something to your bike) and rule breaking
that is brilliantly imaginative (discarding Euclidean space).Hacking predates computers.  When he
was working on the Manhattan Project, Richard Feynman used to
amuse himself by breaking into safes containing secret documents.
This tradition continues today.
When we were in grad school, a hacker friend of mine who spent too much
time around MIT had
his own lock picking kit.
(He now runs a hedge fund, a not unrelated enterprise.)It is sometimes hard to explain to authorities why one would
want to do such things.
Another friend of mine once got in trouble with the government for
breaking into computers.  This had only recently been declared
a crime, and the FBI found that their usual investigative
technique didn't work.  Police investigation apparently begins with
a motive.  The usual motives are few: drugs, money, sex,
revenge.  Intellectual curiosity was not one of the motives on
the FBI's list.  Indeed, the whole concept seemed foreign to
them.Those in authority tend to be annoyed by hackers'
general attitude of disobedience.  But that disobedience is
a byproduct of the qualities that make them good programmers.
They may laugh at the CEO when he talks in generic corporate
newspeech, but they also laugh at someone who tells them
a certain problem can't be solved.
Suppress one, and you suppress the other.This attitude is sometimes affected.  Sometimes young programmers
notice the eccentricities of eminent hackers and decide to
adopt some of their own in order to seem smarter.
The fake version is not merely
annoying; the prickly attitude of these posers
can actually slow the process of innovation.But even factoring in their annoying eccentricities,
the disobedient attitude of hackers is a net win.  I wish its
advantages were better understood.For example, I suspect people in Hollywood are
simply mystified by
hackers' attitudes toward copyrights.  They are a perennial
topic of heated discussion on Slashdot.
But why should people who program computers
be so concerned about copyrights, of all things?Partly because some companies use mechanisms to prevent
copying.  Show any hacker a lock and his first thought is
how to pick it.  But there is a deeper reason that
hackers are alarmed by measures like copyrights and patents.
They see increasingly aggressive measures to protect
"intellectual property"
as a threat to the intellectual
freedom they need to do their job.
And they are right.It is by poking about inside current technology that
hackers get ideas for the next generation.  No thanks,
intellectual homeowners may say, we don't need any
outside help.  But they're wrong.
The next generation of computer technology has
often—perhaps more often than not—been developed by outsiders.In 1977 there was no doubt some group within IBM developing
what they expected to be
the next generation of business computer.  They were mistaken.
The next generation of business computer was
being developed on entirely different lines by two long-haired
guys called Steve in a garage in Los Altos.  At about the
same time, the powers that be
were cooperating to develop the
official next generation operating system, Multics.
But two guys who thought Multics excessively complex went off
and wrote their own.  They gave it a name that
was a joking reference to Multics: Unix.The latest intellectual property laws impose
unprecedented restrictions on the sort of poking around that
leads to new ideas. In the past, a competitor might use patents
to prevent you from selling a copy of something they
made, but they couldn't prevent you from
taking one apart to see how it worked.   The latest
laws make this a crime.  How are we
to develop new technology if we can't study current
technology to figure out how to improve it?Ironically, hackers have brought this on themselves.
Computers are responsible for the problem.  The control systems
inside machines used to be physical: gears and levers and cams.
Increasingly, the brains (and thus the value) of products is
in software. And by this I mean software in the general sense:
i.e. data.  A song on an LP is physically stamped into the
plastic.  A song on an iPod's disk is merely stored on it.Data is by definition easy to copy.  And the Internet
makes copies easy to distribute.  So it is no wonder
companies are afraid.  But, as so often happens, fear has
clouded their judgement.  The government has responded
with draconian laws to protect intellectual property.
They probably mean well. But
they may not realize that such laws will do more harm
than good.Why are programmers so violently opposed to these laws?
If I were a legislator, I'd be interested in this
mystery—for the same reason that, if I were a farmer and suddenly
heard a lot of squawking coming from my hen house one night,
I'd want to go out and investigate.  Hackers are not stupid,
and unanimity is very rare in this world.
So if they're all squawking,   
perhaps there is something amiss.Could it be that such laws, though intended to protect America,
will actually harm it?  Think about it.  There is something
very American about Feynman breaking into safes during
the Manhattan Project.  It's hard to imagine the authorities
having a sense of humor about such things over
in Germany at that time.  Maybe it's not a coincidence.Hackers are unruly.  That is the essence of hacking.  And it
is also the essence of Americanness.  It is no accident
that Silicon Valley
is in America, and not France, or Germany,
or England, or Japan. In those countries, people color inside
the lines.I lived for a while in Florence.  But after I'd been there
a few months I realized that what I'd been unconsciously hoping
to find there was back in the place I'd just left.
The reason Florence is famous is that in 1450, it was New York.
In 1450 it was filled with the kind of turbulent and ambitious
people you find now in America.  (So I went back to America.)It is greatly to America's advantage that it is
a congenial atmosphere for the right sort of unruliness—that
it is a home not just for the smart, but for smart-alecks.
And hackers are invariably smart-alecks.  If we had a national
holiday, it would be April 1st.  It says a great deal about
our work that we use the same word for a brilliant or a
horribly cheesy solution.   When we cook one up we're not
always 100% sure which kind it is.  But as long as it has
the right sort of wrongness, that's a promising sign.
It's odd that people
think of programming as precise and methodical.  Computers
are precise and methodical.  Hacking is something you do
with a gleeful laugh.In our world some of the most characteristic solutions
are not far removed from practical
jokes.  IBM was no doubt rather surprised by the consequences
of the licensing deal for DOS, just as the hypothetical
"adversary" must be when Michael Rabin solves a problem by
redefining it as one that's easier to solve.Smart-alecks have to develop a keen sense of how much they
can get away with.  And lately hackers 
have sensed a change
in the atmosphere.
Lately hackerliness seems rather frowned upon.To hackers the recent contraction in civil liberties seems
especially ominous.  That must also mystify outsiders. 
Why should we care especially about civil
liberties?  Why programmers, more than
dentists or salesmen or landscapers?Let me put the case in terms a government official would appreciate.
Civil liberties are not just an ornament, or a quaint
American tradition.  Civil liberties make countries rich.
If you made a graph of
GNP per capita vs. civil liberties, you'd notice a definite
trend.  Could civil liberties really be a cause, rather
than just an effect?  I think so.  I think a society in which
people can do and say what they want will also tend to
be one in which the most efficient solutions win, rather than
those sponsored by the most influential people.
Authoritarian countries become corrupt;
corrupt countries become poor; and poor countries are weak. 
It seems to me there is
a Laffer curve for government power, just as for
tax revenues.  At least, it seems likely enough that it
would be stupid to try the experiment and find out.  Unlike
high tax rates, you can't repeal totalitarianism if it
turns out to be a mistake.This is why hackers worry.  The government spying on people doesn't
literally make programmers write worse code.  It just leads
eventually to a world in which bad ideas win.  And because
this is so important to hackers, they're especially sensitive
to it.  They can sense totalitarianism approaching from a
distance, as animals can sense an approaching  
thunderstorm.It would be ironic if, as hackers fear, recent measures
intended to protect national security and intellectual property
turned out to be a missile aimed right at what makes   
America successful.  But it would not be the first time that
measures taken in an atmosphere of panic had
the opposite of the intended effect.There is such a thing as Americanness.
There's nothing like living abroad to teach you that.   
And if you want to know whether something will nurture or squash
this quality, it would be hard to find a better focus
group than hackers, because they come closest of any group
I know to embodying it.  Closer, probably,  than
the men running our government,
who for all their talk of patriotism
remind me more of Richelieu or Mazarin
than Thomas Jefferson or George Washington.When you read what the founding fathers had to say for
themselves, they sound more like hackers.
"The spirit of resistance to government,"
Jefferson wrote, "is so valuable on certain occasions, that I wish
it always to be kept alive."Imagine an American president saying that today.
Like the remarks of an outspoken old grandmother, the sayings of
the founding fathers have embarrassed generations of
their less confident successors.  They remind us where we come from.
They remind us that it is the people who break rules that are
the source of America's wealth and power.Those in a position to impose rules naturally want them to be
obeyed.  But be careful what you ask for. You might get it.Thanks to Ken Anderson, Trevor Blackwell, Daniel Giffin, 
Sarah Harlin,  Shiro Kawai, Jessica Livingston, Matz, 
Jackie McDonough, Robert Morris, Eric Raymond, Guido van Rossum,
David Weinberger, and
Steven Wolfram for reading drafts of this essay.
(The image shows Steves Jobs and Wozniak 
with a "blue box."
Photo by Margret Wozniak. Reproduced by permission of Steve
Wozniak.)

Want to start a startup?  Get funded by
Y Combinator.




July 2004(This essay is derived from a talk at Oscon 2004.)
A few months ago I finished a new 
book, 
and in reviews I keep
noticing words like "provocative'' and "controversial.'' To say
nothing of "idiotic.''I didn't mean to make the book controversial.  I was trying to make
it efficient.  I didn't want to waste people's time telling them
things they already knew.  It's more efficient just to give them
the diffs.  But I suppose that's bound to yield an alarming book.EdisonsThere's no controversy about which idea is most controversial:
the suggestion that variation in wealth might not be as big a
problem as we think.I didn't say in the book that variation in wealth was in itself a
good thing.  I said in some situations it might be a sign of good
things.  A throbbing headache is not a good thing, but it can be
a sign of a good thing-- for example, that you're recovering
consciousness after being hit on the head.Variation in wealth can be a sign of variation in productivity.
(In a society of one, they're identical.) And that
is almost certainly a good thing: if your society has no variation
in productivity, it's probably not because everyone is Thomas
Edison.  It's probably because you have no Thomas Edisons.In a low-tech society you don't see much variation in productivity.
If you have a tribe of nomads collecting sticks for a fire, how
much more productive is the best stick gatherer going to be than
the worst?  A factor of two?  Whereas when you hand people a complex tool
like a computer, the variation in what they can do with
it is enormous.That's not a new idea.  Fred Brooks wrote about it in 1974, and
the study he quoted was published in 1968.  But I think he
underestimated the variation between programmers.  He wrote about productivity in lines
of code:  the best programmers can solve a given problem in a tenth
the time.  But what if the problem isn't given? In programming, as
in many fields, the hard part isn't solving problems, but deciding
what problems to solve.  Imagination is hard to measure, but
in practice it dominates the kind of productivity that's measured
in lines of code.Productivity varies in any field, but there are few in which it
varies so much.  The variation between programmers
is so great that it becomes a difference in kind.  I don't
think this is something intrinsic to programming, though.  In every field,
technology magnifies differences in productivity.  I think what's
happening in programming is just that we have a lot of technological
leverage.  But in every field the lever is getting longer, so the
variation we see is something that more and more fields will see
as time goes on.  And the success of companies, and countries, will
depend increasingly on how they deal with it.If variation in productivity increases with technology, then the
contribution of the most productive individuals will not only be
disproportionately large, but will actually grow with time.  When
you reach the point where 90% of a group's output is created by 1%
of its members, you lose big if something (whether Viking raids,
or central planning) drags their productivity down to the average.If we want to get the most out of them, we need to understand these
especially productive people.  What motivates them?  What do they
need to do their jobs?  How do you recognize them? How do you
get them to come and work for you?  And then of course there's the
question, how do you become one?More than MoneyI know a handful of super-hackers, so I sat down and thought about
what they have in common.  Their defining quality is probably that
they really love to program.  Ordinary programmers write code to pay
the bills.  Great hackers think of it as something they do for fun,
and which they're delighted to find people will pay them for.Great programmers are sometimes said to be indifferent to money.
This isn't quite true.  It is true that all they really care about
is doing interesting work.  But if you make enough money, you get
to work on whatever you want, and for that reason hackers are
attracted by the idea of making really large amounts of money.
But as long as they still have to show up for work every day, they
care more about what they do there than how much they get paid for
it.Economically, this is a fact of the greatest importance, because
it means you don't have to pay great hackers anything like what
they're worth.  A great programmer might be ten or a hundred times
as productive as an ordinary one, but he'll consider himself lucky
to get paid three times as much.  As I'll explain later, this is
partly because great hackers don't know how good they are.  But
it's also because money is not the main thing they want.What do hackers want?  Like all craftsmen, hackers like good tools.
In fact, that's an understatement.  Good hackers find it unbearable
to use bad tools.  They'll simply refuse to work on projects with
the wrong infrastructure.At a startup I once worked for, one of the things pinned up on our
bulletin board was an ad from IBM.  It was a picture of an AS400,
and the headline read, I think, "hackers despise
it.'' [1]When you decide what infrastructure to use for a project, you're
not just making a technical decision.  You're also making a social
decision, and this may be the more important of the two.  For
example, if your company wants to write some software, it might
seem a prudent choice to write it in Java.  But when you choose a
language, you're also choosing a community.  The programmers you'll
be able to hire to work on a Java project won't be as
smart as the
ones you could get to work on a project written in Python.
And the quality of your hackers probably matters more than the
language you choose.  Though, frankly, the fact that good hackers
prefer Python to Java should tell you something about the relative
merits of those languages.Business types prefer the most popular languages because they view
languages as standards. They don't want to bet the company on
Betamax.  The thing about languages, though, is that they're not
just standards.  If you have to move bits over a network, by all
means use TCP/IP.  But a programming language isn't just a format.
A programming language is a medium of expression.I've read that Java has just overtaken Cobol as the most popular
language.  As a standard, you couldn't wish for more.  But as a
medium of expression, you could do a lot better.  Of all the great
programmers I can think of, I know of only one who would voluntarily
program in Java.  And of all the great programmers I can think of
who don't work for Sun, on Java, I know of zero.Great hackers also generally insist on using open source software.
Not just because it's better, but because it gives them more control.
Good hackers insist on control.  This is part of what makes them
good hackers:  when something's broken, they need to fix it.  You
want them to feel this way about the software they're writing for
you.  You shouldn't be surprised when they feel the same way about
the operating system.A couple years ago a venture capitalist friend told me about a new
startup he was involved with.  It sounded promising.  But the next
time I talked to him, he said they'd decided to build their software
on Windows NT, and had just hired a very experienced NT developer
to be their chief technical officer.  When I heard this, I thought,
these guys are doomed.  One, the CTO couldn't be a first rate
hacker, because to become an eminent NT developer he would have
had to use NT voluntarily, multiple times, and I couldn't imagine
a great hacker doing that; and two, even if he was good, he'd have
a hard time hiring anyone good to work for him if the project had
to be built on NT. [2]The Final FrontierAfter software, the most important tool to a hacker is probably
his office.  Big companies think the function of office space is to express
rank.  But hackers use their offices for more than that: they
use their office as a place to think in.  And if you're a technology
company, their thoughts are your product.  So making hackers work
in a noisy, distracting environment is like having a paint factory
where the air is full of soot.The cartoon strip Dilbert has a lot to say about cubicles, and with
good reason.  All the hackers I know despise them.  The mere prospect
of being interrupted is enough to prevent hackers from working on
hard problems.  If you want to get real work done in an office with
cubicles, you have two options: work at home, or come in early or
late or on a weekend, when no one else is there.  Don't companies
realize this is a sign that something is broken?  An office
environment is supposed to be something that helps
you work, not something you work despite.Companies like Cisco are proud that everyone there has a cubicle,
even the CEO.  But they're not so advanced as they think; obviously
they still view office space as a badge of rank.  Note too that
Cisco is famous for doing very little product development in house.
They get new technology by buying the startups that created it-- where
presumably the hackers did have somewhere quiet to work.One big company that understands what hackers need is Microsoft.
I once saw a recruiting ad for Microsoft with a big picture of a
door.  Work for us, the premise was, and we'll give you a place to
work where you can actually get work done.   And you know, Microsoft
is remarkable among big companies in that they are able to develop
software in house.  Not well, perhaps, but well enough.If companies want hackers to be productive, they should look at
what they do at home.  At home, hackers can arrange things themselves
so they can get the most done.  And when they work at home, hackers
don't work in noisy, open spaces; they work in rooms with doors.  They
work in cosy, neighborhoody places with people around and somewhere
to walk when they need to mull something over, instead of in glass
boxes set in acres of parking lots.  They have a sofa they can take
a nap on when they feel tired, instead of sitting in a coma at
their desk, pretending to work.  There's no crew of people with
vacuum cleaners that roars through every evening during the prime
hacking hours.  There are no meetings or, God forbid, corporate
retreats or team-building exercises.  And when you look at what
they're doing on that computer, you'll find it reinforces what I
said earlier about tools.  They may have to use Java and Windows
at work, but at home, where they can choose for themselves, you're
more likely to find them using Perl and Linux.Indeed, these statistics about Cobol or Java being the most popular
language can be misleading.  What we ought to look at, if we want
to know what tools are best, is what hackers choose when they can
choose freely-- that is, in projects of their own.  When you ask
that question, you find that open source operating systems already
have a dominant market share, and the number one language is probably
Perl.InterestingAlong with good tools, hackers want interesting projects.  What
makes a project interesting?  Well, obviously overtly sexy
applications like stealth planes or special effects software would
be interesting to work on.  But any application can be interesting
if it poses novel technical challenges.  So it's hard to predict
which problems hackers will like, because some become
interesting only when the people working on them discover a new
kind of solution.  Before ITA
(who wrote the software inside Orbitz),
the people working on airline fare searches probably thought it
was one of the most boring applications imaginable.  But ITA made
it interesting by 
redefining the problem in a more ambitious way.I think the same thing happened at Google.  When Google was founded,
the conventional wisdom among the so-called portals was that search
was boring and unimportant.  But the guys at Google didn't think
search was boring, and that's why they do it so well.This is an area where managers can make a difference.  Like a parent
saying to a child, I bet you can't clean up your whole room in
ten minutes, a good manager can sometimes redefine a problem as a
more interesting one.  Steve Jobs seems to be particularly good at
this, in part simply by having high standards.  There were a lot
of small, inexpensive computers before the Mac.  He redefined the
problem as: make one that's beautiful.  And that probably drove
the developers harder than any carrot or stick could.They certainly delivered.  When the Mac first appeared, you didn't
even have to turn it on to know it would be good; you could tell
from the case.  A few weeks ago I was walking along the street in
Cambridge, and in someone's trash I saw what appeared to be a Mac
carrying case.  I looked inside, and there was a Mac SE.  I carried
it home and plugged it in, and it booted.  The happy Macintosh
face, and then the finder.  My God, it was so simple.  It was just
like... Google.Hackers like to work for people with high standards.  But it's not
enough just to be exacting.  You have to insist on the right things.
Which usually means that you have to be a hacker yourself.  I've
seen occasional articles about how to manage programmers.  Really
there should be two articles: one about what to do if
you are yourself a programmer, and one about what to do if you're not.  And the 
second could probably be condensed into two words:  give up.The problem is not so much the day to day management.  Really good
hackers are practically self-managing.  The problem is, if you're
not a hacker, you can't tell who the good hackers are.  A similar
problem explains why American cars are so ugly.  I call it the
design paradox.  You might think that you could make your products
beautiful just by hiring a great designer to design them.  But if
you yourself don't have good taste, 
how are you going to recognize
a good designer?  By definition you can't tell from his portfolio.
And you can't go by the awards he's won or the jobs he's had,
because in design, as in most fields, those tend to be driven by
fashion and schmoozing, with actual ability a distant third.
There's no way around it:  you can't manage a process intended to
produce beautiful things without knowing what beautiful is.  American
cars are ugly because American car companies are run by people with
bad taste.Many people in this country think of taste as something elusive,
or even frivolous.  It is neither.  To drive design, a manager must
be the most demanding user of a company's products.  And if you
have really good taste, you can, as Steve Jobs does, make satisfying
you the kind of problem that good people like to work on.Nasty Little ProblemsIt's pretty easy to say what kinds of problems are not interesting:
those where instead of solving a few big, clear, problems, you have
to solve a lot of nasty little ones.  One of the worst kinds of
projects is writing an interface to a piece of software that's
full of bugs.  Another is when you have to customize
something for an individual client's complex and ill-defined needs.
To hackers these kinds of projects are the death of a thousand
cuts.The distinguishing feature of nasty little problems is that you
don't learn anything from them.   Writing a compiler is interesting
because it teaches you what a compiler is.  But writing an interface
to a buggy piece of software doesn't teach you anything, because the
bugs are random.  [3] So it's not just fastidiousness that makes good
hackers avoid nasty little problems.  It's more a question of
self-preservation.  Working on nasty little problems makes you
stupid.  Good hackers avoid it for the same reason models avoid
cheeseburgers.Of course some problems inherently have this character.  And because
of supply and demand, they pay especially well.  So a company that
found a way to get great hackers to work on tedious problems would
be very successful.  How would you do it?One place this happens is in startups.  At our startup we had 
Robert Morris working as a system administrator.  That's like having the
Rolling Stones play at a bar mitzvah.  You can't hire that kind of
talent.  But people will do any amount of drudgery for companies
of which they're the founders.  [4]Bigger companies solve the problem by partitioning the company.
They get smart people to work for them by establishing a separate
R&D department where employees don't have to work directly on
customers' nasty little problems. [5] In this model, the research
department functions like a mine. They produce new ideas; maybe
the rest of the company will be able to use them.You may not have to go to this extreme.  
Bottom-up programming
suggests another way to partition the company: have the smart people
work as toolmakers.  If your company makes software to do x, have
one group that builds tools for writing software of that type, and
another that uses these tools to write the applications.  This way
you might be able to get smart people to write 99% of your code,
but still keep them almost as insulated from users as they would
be in a traditional research department.  The toolmakers would have
users, but they'd only be the company's own developers.  [6]If Microsoft used this approach, their software wouldn't be so full
of security holes, because the less smart people writing the actual
applications wouldn't be doing low-level stuff like allocating
memory.  Instead of writing Word directly in C, they'd be plugging
together big Lego blocks of Word-language.  (Duplo, I believe, is
the technical term.)ClumpingAlong with interesting problems, what good hackers like is other
good hackers.  Great hackers tend to clump together-- sometimes
spectacularly so, as at Xerox Parc.   So you won't attract good
hackers in linear proportion to how good an environment you create
for them.  The tendency to clump means it's more like the square
of the environment.  So it's winner take all.  At any given time,
there are only about ten or twenty places where hackers most want to
work, and if you aren't one of them, you won't just have fewer
great hackers, you'll have zero.Having great hackers is not, by itself, enough to make a company
successful.  It works well for Google and ITA, which are two of
the hot spots right now, but it didn't help Thinking Machines or
Xerox.  Sun had a good run for a while, but their business model
is a down elevator.  In that situation, even the best hackers can't
save you.I think, though, that all other things being equal, a company that
can attract great hackers will have a huge advantage.  There are
people who would disagree with this.  When we were making the rounds
of venture capital firms in the 1990s, several told us that software
companies didn't win by writing great software, but through brand,
and dominating channels, and doing the right deals.They really seemed to believe this, and I think I know why.  I
think what a lot of VCs are looking for, at least unconsciously,
is the next Microsoft.  And of course if Microsoft is your model,
you shouldn't be looking for companies that hope to win by writing
great software.  But VCs are mistaken to look for the next Microsoft,
because no startup can be the next Microsoft unless some other
company is prepared to bend over at just the right moment and be
the next IBM.It's a mistake to use Microsoft as a model, because their whole
culture derives from that one lucky break.  Microsoft is a bad data
point.  If you throw them out, you find that good products do tend
to win in the market.  What VCs should be looking for is the next
Apple, or the next Google.I think Bill Gates knows this.  What worries him about Google is
not the power of their brand, but the fact that they have
better hackers. [7]
RecognitionSo who are the great hackers?  How do you know when you meet one?
That turns out to be very hard.  Even hackers can't tell.  I'm
pretty sure now that my friend Trevor Blackwell is a great hacker.
You may have read on Slashdot how he made his 
own Segway.  The
remarkable thing about this project was that he wrote all the
software in one day (in Python, incidentally).For Trevor, that's
par for the course.  But when I first met him, I thought he was a
complete idiot.  He was standing in Robert Morris's office babbling
at him about something or other, and I remember standing behind
him making frantic gestures at Robert to shoo this nut out of his
office so we could go to lunch.  Robert says he misjudged Trevor
at first too.  Apparently when Robert first met him, Trevor had
just begun a new scheme that involved writing down everything about
every aspect of his life on a stack of index cards, which he carried
with him everywhere.  He'd also just arrived from Canada, and had
a strong Canadian accent and a mullet.The problem is compounded by the fact that hackers, despite their
reputation for social obliviousness, sometimes put a good deal of
effort into seeming smart.  When I was in grad school I used to
hang around the MIT AI Lab occasionally. It was kind of intimidating
at first.  Everyone there spoke so fast.  But after a while I
learned the trick of speaking fast.  You don't have to think any
faster; just use twice as many words to say everything.  With this amount of noise in the signal, it's hard to tell good
hackers when you meet them.  I can't tell, even now.  You also
can't